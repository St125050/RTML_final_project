{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba01ff8a-4999-47ef-ac23-8ee888cf8112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://images.cocodataset.org/annotations/annotations_trainval2017.zip...\n",
      "loading annotations into memory...\n",
      "Done (t=11.21s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [25:27<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from pycocotools.coco import COCO\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download COCO dataset annotations\n",
    "annotations_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "annotations_path = \"annotations_trainval2017.zip\"\n",
    "annotations_dir = \"annotations\"\n",
    "\n",
    "if not os.path.exists(annotations_path):\n",
    "    print(f\"Downloading {annotations_url}...\")\n",
    "    response = requests.get(annotations_url)\n",
    "    with open(annotations_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    # Extract the zip file\n",
    "    with ZipFile(annotations_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(annotations_dir)\n",
    "\n",
    "# Load annotations\n",
    "coco = COCO(os.path.join(annotations_dir, \"annotations/instances_train2017.json\"))\n",
    "\n",
    "# Select 1,000 images\n",
    "image_ids = coco.getImgIds()[:1000]\n",
    "\n",
    "# Function to download images from COCO\n",
    "def download_coco_images(coco, img_ids, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    for img_id in tqdm(img_ids):\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_url = img_info[\"coco_url\"]\n",
    "        img_path = os.path.join(save_dir, img_info[\"file_name\"])\n",
    "        if not os.path.exists(img_path):\n",
    "            response = requests.get(img_url)\n",
    "            with open(img_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "# Download images\n",
    "download_coco_images(coco, image_ids, \"coco_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1852fadd-042d-4358-9b84-cd66daeff907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in ./.local/lib/python3.12/site-packages (from pycocotools) (3.10.0)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.12/site-packages (from pycocotools) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.local/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.local/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in ./.local/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.local/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/tljh/user/lib/python3.12/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n",
      "Downloading pycocotools-2.0.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
      "Installing collected packages: pycocotools\n",
      "Successfully installed pycocotools-2.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6310f303-91a3-40db-99f3-da76a38bb452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.12/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.12/site-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/tljh/user/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/tljh/user/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/tljh/user/lib/python3.12/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.local/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./.local/lib/python3.12/site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./.local/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/tljh/user/lib/python3.12/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/tljh/user/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/tljh/user/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/tljh/user/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "675ece14-e83f-4b8f-a1a9-36fa8561f500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 1000/1000 [00:00<00:00, 72460.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared and saved to 'coco_dataset.csv'\n",
      "                     image_path  \\\n",
      "0  coco_images/000000391895.jpg   \n",
      "1  coco_images/000000522418.jpg   \n",
      "2  coco_images/000000184613.jpg   \n",
      "3  coco_images/000000318219.jpg   \n",
      "4  coco_images/000000554625.jpg   \n",
      "\n",
      "                                             caption  \n",
      "0  A man with a red helmet on a small moped on a ...  \n",
      "1  A woman wearing a net on her head cutting a ca...  \n",
      "2  A child holding a flowered umbrella and pettin...  \n",
      "3  A young boy standing in front of a computer ke...  \n",
      "4  a boy wearing headphones using one computer in...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load COCO captions annotations\n",
    "annotations_dir = \"annotations\"\n",
    "captions_file = os.path.join(annotations_dir, \"annotations/captions_train2017.json\")\n",
    "coco = COCO(captions_file)\n",
    "\n",
    "# Select 1,000 image IDs (same as before)\n",
    "image_ids = coco.getImgIds()[:1000]\n",
    "\n",
    "# Create a DataFrame with image paths and captions\n",
    "data = []\n",
    "for img_id in tqdm(image_ids, desc=\"Processing images\"):\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)  # Get annotation IDs for this image\n",
    "    anns = coco.loadAnns(ann_ids)  # Load annotations (captions)\n",
    "    caption = anns[0][\"caption\"] if anns else \"No caption available\"  # Use first caption\n",
    "    data.append({\"image_path\": os.path.join(\"coco_images\", img_info[\"file_name\"]), \"caption\": caption})\n",
    "\n",
    "# Save to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"coco_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Dataset prepared and saved to 'coco_dataset.csv'\")\n",
    "print(df.head())  # Preview the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0476fe-ac26-44bd-b030-6abfff3196f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[\"image_path\"][idx]\n",
    "        caption = self.data[\"caption\"][idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        text = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "        return image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = CocoDataset(\"coco_dataset.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1eee3f-63c6-4b79-9929-6234773db9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 CUDA GPUs.\n",
      "GPU 0: Total Memory = 10.75 GiB, Free Memory = 9.72 GiB\n",
      "GPU 1: Total Memory = 10.75 GiB, Free Memory = 10.75 GiB\n",
      "GPU 2: Total Memory = 10.75 GiB, Free Memory = 10.75 GiB\n",
      "GPU 3: Total Memory = 10.75 GiB, Free Memory = 10.75 GiB\n",
      "Selecting GPU 1 with 10.75 GiB free memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/deit-small-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "CLIP Epoch 1:  80%|███████▉  | 399/500 [00:33<00:08, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 1: 100%|██████████| 500/500 [00:42<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 1, Average Loss: 10.8808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 2:  67%|██████▋   | 337/500 [00:27<00:13, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 2: 100%|██████████| 500/500 [00:41<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 2, Average Loss: 1.1382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 3:  88%|████████▊ | 441/500 [00:37<00:04, 14.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 3: 100%|██████████| 500/500 [00:42<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 3, Average Loss: 0.8262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 4:  26%|██▌       | 130/500 [00:11<00:30, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 4: 100%|██████████| 500/500 [00:42<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 4, Average Loss: 0.7509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 5:  93%|█████████▎| 466/500 [00:38<00:02, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 5: 100%|██████████| 500/500 [00:41<00:00, 11.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 5, Average Loss: 0.7471\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import ViTModel, DistilBertModel, DistilBertTokenizer  # Updated to DistilBertModel\n",
    "from torchvision import transforms\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to select best GPU\n",
    "def get_best_gpu():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No CUDA GPUs available. Falling back to CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus == 0:\n",
    "        print(\"No GPUs detected. Falling back to CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "    print(f\"Detected {num_gpus} CUDA GPUs.\")\n",
    "    free_memory = []\n",
    "    for i in range(num_gpus):\n",
    "        torch.cuda.set_device(i)\n",
    "        total_memory = torch.cuda.get_device_properties(i).total_memory\n",
    "        allocated_memory = torch.cuda.memory_allocated(i)\n",
    "        reserved_memory = torch.cuda.memory_reserved(i)\n",
    "        free_mem = total_memory - (allocated_memory + reserved_memory)\n",
    "        free_memory.append((i, free_mem))\n",
    "        print(f\"GPU {i}: Total Memory = {total_memory / 1024**3:.2f} GiB, \"\n",
    "              f\"Free Memory = {free_mem / 1024**3:.2f} GiB\")\n",
    "    \n",
    "    best_gpu_idx, best_free_mem = max(free_memory, key=lambda x: x[1])\n",
    "    print(f\"Selecting GPU {best_gpu_idx} with {best_free_mem / 1024**3:.2f} GiB free memory.\")\n",
    "    return torch.device(f\"cuda:{best_gpu_idx}\")\n",
    "\n",
    "# Dataset with error handling\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.bad_images = []  # Track problematic images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[\"image_path\"][idx]\n",
    "        caption = self.data[\"caption\"][idx]\n",
    "        \n",
    "        # Handle image loading with error checking\n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, FileNotFoundError, Exception) as e:\n",
    "            print(f\"Warning: Skipping {img_path} due to error: {e}\")\n",
    "            self.bad_images.append(img_path)\n",
    "            # Return a dummy image and text to keep the batch size consistent\n",
    "            dummy_image = torch.zeros(3, 224, 224)  # Dummy tensor for invalid images\n",
    "            text = self.tokenizer(\"Invalid image\", return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "            return dummy_image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        text = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "        return image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "dataset = CocoDataset(\"coco_dataset.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)  # Reduced batch size\n",
    "\n",
    "# CLIP Model\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIP, self).__init__()\n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"facebook/deit-small-patch16-224\")\n",
    "        self.text_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")  # Updated to DistilBertModel\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(384, self.projection_dim)  # DeiT-Small outputs 384-dim\n",
    "        self.text_proj = nn.Linear(768, self.projection_dim)    # DistilBERT outputs 768-dim\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_outputs = self.vision_encoder(images).last_hidden_state[:, 0, :]\n",
    "        text_outputs = self.text_encoder(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        vision_embeds = self.vision_proj(vision_outputs)\n",
    "        text_embeds = self.text_proj(text_outputs)\n",
    "        return vision_embeds, text_embeds\n",
    "\n",
    "def contrastive_loss(vision_embeds, text_embeds, temperature=0.07):\n",
    "    logits = torch.matmul(vision_embeds, text_embeds.T) / temperature\n",
    "    labels = torch.arange(len(vision_embeds)).to(vision_embeds.device)\n",
    "    loss_i = nn.CrossEntropyLoss()(logits, labels)\n",
    "    loss_t = nn.CrossEntropyLoss()(logits.T, labels)\n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "# Training\n",
    "device = get_best_gpu()\n",
    "clip_model = CLIP().to(device)\n",
    "optimizer = torch.optim.Adam(clip_model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Gradient accumulation settings\n",
    "accumulation_steps = 4\n",
    "\n",
    "for epoch in range(5):\n",
    "    clip_model.train()\n",
    "    total_loss = 0\n",
    "    for i, (images, input_ids, attention_mask) in enumerate(tqdm(dataloader, desc=f\"CLIP Epoch {epoch+1}\")):\n",
    "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            vision_embeds, text_embeds = clip_model(images, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(vision_embeds, text_embeds)\n",
    "            loss = loss / accumulation_steps  # Normalize loss for gradient accumulation\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps  # Accumulate total loss\n",
    "    \n",
    "    print(f\"CLIP Epoch {epoch+1}, Average Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    # Log bad images after each epoch\n",
    "    if dataset.bad_images:\n",
    "        print(f\"Bad images skipped in Epoch {epoch+1}: {dataset.bad_images}\")\n",
    "\n",
    "torch.save(clip_model.state_dict(), \"clip_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0601374-e5b1-42b7-b8f7-02281c9d5dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1fd412ef57440b93de4dccc9ea767b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3aad29c8b14a6bad07b716d34ee217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d7ebdae79840feaacc250fc6978eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60c1515ea074340b56bf54ccbc866c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d7e160c15c48c1a86c055f0ebe0fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865d6f7b8bf340f7bb4b288e98ec0479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2cce6c4178945c6abe870bf37e858af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipForImageTextRetrieval were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['itm_head.bias', 'itm_head.weight', 'text_encoder.embeddings.LayerNorm.bias', 'text_encoder.embeddings.LayerNorm.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.word_embeddings.weight', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.attention.output.dense.bias', 'text_encoder.encoder.layer.0.attention.output.dense.weight', 'text_encoder.encoder.layer.0.attention.self.key.bias', 'text_encoder.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.0.attention.self.query.weight', 'text_encoder.encoder.layer.0.attention.self.value.bias', 'text_encoder.encoder.layer.0.attention.self.value.weight', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.crossattention.output.dense.bias', 'text_encoder.encoder.layer.0.crossattention.output.dense.weight', 'text_encoder.encoder.layer.0.crossattention.self.key.bias', 'text_encoder.encoder.layer.0.crossattention.self.key.weight', 'text_encoder.encoder.layer.0.crossattention.self.query.bias', 'text_encoder.encoder.layer.0.crossattention.self.query.weight', 'text_encoder.encoder.layer.0.crossattention.self.value.bias', 'text_encoder.encoder.layer.0.crossattention.self.value.weight', 'text_encoder.encoder.layer.0.intermediate.dense.bias', 'text_encoder.encoder.layer.0.intermediate.dense.weight', 'text_encoder.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.output.dense.bias', 'text_encoder.encoder.layer.0.output.dense.weight', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.attention.output.dense.bias', 'text_encoder.encoder.layer.1.attention.output.dense.weight', 'text_encoder.encoder.layer.1.attention.self.key.bias', 'text_encoder.encoder.layer.1.attention.self.key.weight', 'text_encoder.encoder.layer.1.attention.self.query.bias', 'text_encoder.encoder.layer.1.attention.self.query.weight', 'text_encoder.encoder.layer.1.attention.self.value.bias', 'text_encoder.encoder.layer.1.attention.self.value.weight', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder.encoder.layer.1.crossattention.output.dense.weight', 'text_encoder.encoder.layer.1.crossattention.self.key.bias', 'text_encoder.encoder.layer.1.crossattention.self.key.weight', 'text_encoder.encoder.layer.1.crossattention.self.query.bias', 'text_encoder.encoder.layer.1.crossattention.self.query.weight', 'text_encoder.encoder.layer.1.crossattention.self.value.bias', 'text_encoder.encoder.layer.1.crossattention.self.value.weight', 'text_encoder.encoder.layer.1.intermediate.dense.bias', 'text_encoder.encoder.layer.1.intermediate.dense.weight', 'text_encoder.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.output.dense.bias', 'text_encoder.encoder.layer.1.output.dense.weight', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.attention.output.dense.bias', 'text_encoder.encoder.layer.10.attention.output.dense.weight', 'text_encoder.encoder.layer.10.attention.self.key.bias', 'text_encoder.encoder.layer.10.attention.self.key.weight', 'text_encoder.encoder.layer.10.attention.self.query.bias', 'text_encoder.encoder.layer.10.attention.self.query.weight', 'text_encoder.encoder.layer.10.attention.self.value.bias', 'text_encoder.encoder.layer.10.attention.self.value.weight', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder.encoder.layer.10.crossattention.self.key.bias', 'text_encoder.encoder.layer.10.crossattention.self.key.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.bias', 'text_encoder.encoder.layer.10.crossattention.self.query.weight', 'text_encoder.encoder.layer.10.crossattention.self.value.bias', 'text_encoder.encoder.layer.10.crossattention.self.value.weight', 'text_encoder.encoder.layer.10.intermediate.dense.bias', 'text_encoder.encoder.layer.10.intermediate.dense.weight', 'text_encoder.encoder.layer.10.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.output.dense.bias', 'text_encoder.encoder.layer.10.output.dense.weight', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.output.dense.bias', 'text_encoder.encoder.layer.11.attention.output.dense.weight', 'text_encoder.encoder.layer.11.attention.self.key.bias', 'text_encoder.encoder.layer.11.attention.self.key.weight', 'text_encoder.encoder.layer.11.attention.self.query.bias', 'text_encoder.encoder.layer.11.attention.self.query.weight', 'text_encoder.encoder.layer.11.attention.self.value.bias', 'text_encoder.encoder.layer.11.attention.self.value.weight', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder.encoder.layer.11.crossattention.self.key.bias', 'text_encoder.encoder.layer.11.crossattention.self.key.weight', 'text_encoder.encoder.layer.11.crossattention.self.query.bias', 'text_encoder.encoder.layer.11.crossattention.self.query.weight', 'text_encoder.encoder.layer.11.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.crossattention.self.value.weight', 'text_encoder.encoder.layer.11.intermediate.dense.bias', 'text_encoder.encoder.layer.11.intermediate.dense.weight', 'text_encoder.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.output.dense.bias', 'text_encoder.encoder.layer.11.output.dense.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.output.dense.bias', 'text_encoder.encoder.layer.2.attention.output.dense.weight', 'text_encoder.encoder.layer.2.attention.self.key.bias', 'text_encoder.encoder.layer.2.attention.self.key.weight', 'text_encoder.encoder.layer.2.attention.self.query.bias', 'text_encoder.encoder.layer.2.attention.self.query.weight', 'text_encoder.encoder.layer.2.attention.self.value.bias', 'text_encoder.encoder.layer.2.attention.self.value.weight', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder.encoder.layer.2.crossattention.output.dense.weight', 'text_encoder.encoder.layer.2.crossattention.self.key.bias', 'text_encoder.encoder.layer.2.crossattention.self.key.weight', 'text_encoder.encoder.layer.2.crossattention.self.query.bias', 'text_encoder.encoder.layer.2.crossattention.self.query.weight', 'text_encoder.encoder.layer.2.crossattention.self.value.bias', 'text_encoder.encoder.layer.2.crossattention.self.value.weight', 'text_encoder.encoder.layer.2.intermediate.dense.bias', 'text_encoder.encoder.layer.2.intermediate.dense.weight', 'text_encoder.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.output.dense.bias', 'text_encoder.encoder.layer.2.output.dense.weight', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.attention.output.dense.bias', 'text_encoder.encoder.layer.3.attention.output.dense.weight', 'text_encoder.encoder.layer.3.attention.self.key.bias', 'text_encoder.encoder.layer.3.attention.self.key.weight', 'text_encoder.encoder.layer.3.attention.self.query.bias', 'text_encoder.encoder.layer.3.attention.self.query.weight', 'text_encoder.encoder.layer.3.attention.self.value.bias', 'text_encoder.encoder.layer.3.attention.self.value.weight', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.crossattention.output.dense.bias', 'text_encoder.encoder.layer.3.crossattention.output.dense.weight', 'text_encoder.encoder.layer.3.crossattention.self.key.bias', 'text_encoder.encoder.layer.3.crossattention.self.key.weight', 'text_encoder.encoder.layer.3.crossattention.self.query.bias', 'text_encoder.encoder.layer.3.crossattention.self.query.weight', 'text_encoder.encoder.layer.3.crossattention.self.value.bias', 'text_encoder.encoder.layer.3.crossattention.self.value.weight', 'text_encoder.encoder.layer.3.intermediate.dense.bias', 'text_encoder.encoder.layer.3.intermediate.dense.weight', 'text_encoder.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.output.dense.bias', 'text_encoder.encoder.layer.3.output.dense.weight', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.attention.output.dense.bias', 'text_encoder.encoder.layer.4.attention.output.dense.weight', 'text_encoder.encoder.layer.4.attention.self.key.bias', 'text_encoder.encoder.layer.4.attention.self.key.weight', 'text_encoder.encoder.layer.4.attention.self.query.bias', 'text_encoder.encoder.layer.4.attention.self.query.weight', 'text_encoder.encoder.layer.4.attention.self.value.bias', 'text_encoder.encoder.layer.4.attention.self.value.weight', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.crossattention.output.dense.bias', 'text_encoder.encoder.layer.4.crossattention.output.dense.weight', 'text_encoder.encoder.layer.4.crossattention.self.key.bias', 'text_encoder.encoder.layer.4.crossattention.self.key.weight', 'text_encoder.encoder.layer.4.crossattention.self.query.bias', 'text_encoder.encoder.layer.4.crossattention.self.query.weight', 'text_encoder.encoder.layer.4.crossattention.self.value.bias', 'text_encoder.encoder.layer.4.crossattention.self.value.weight', 'text_encoder.encoder.layer.4.intermediate.dense.bias', 'text_encoder.encoder.layer.4.intermediate.dense.weight', 'text_encoder.encoder.layer.4.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.4.output.dense.weight', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.attention.output.dense.bias', 'text_encoder.encoder.layer.5.attention.output.dense.weight', 'text_encoder.encoder.layer.5.attention.self.key.bias', 'text_encoder.encoder.layer.5.attention.self.key.weight', 'text_encoder.encoder.layer.5.attention.self.query.bias', 'text_encoder.encoder.layer.5.attention.self.query.weight', 'text_encoder.encoder.layer.5.attention.self.value.bias', 'text_encoder.encoder.layer.5.attention.self.value.weight', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder.encoder.layer.5.crossattention.output.dense.weight', 'text_encoder.encoder.layer.5.crossattention.self.key.bias', 'text_encoder.encoder.layer.5.crossattention.self.key.weight', 'text_encoder.encoder.layer.5.crossattention.self.query.bias', 'text_encoder.encoder.layer.5.crossattention.self.query.weight', 'text_encoder.encoder.layer.5.crossattention.self.value.bias', 'text_encoder.encoder.layer.5.crossattention.self.value.weight', 'text_encoder.encoder.layer.5.intermediate.dense.bias', 'text_encoder.encoder.layer.5.intermediate.dense.weight', 'text_encoder.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.output.dense.bias', 'text_encoder.encoder.layer.5.output.dense.weight', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.attention.output.dense.bias', 'text_encoder.encoder.layer.6.attention.output.dense.weight', 'text_encoder.encoder.layer.6.attention.self.key.bias', 'text_encoder.encoder.layer.6.attention.self.key.weight', 'text_encoder.encoder.layer.6.attention.self.query.bias', 'text_encoder.encoder.layer.6.attention.self.query.weight', 'text_encoder.encoder.layer.6.attention.self.value.bias', 'text_encoder.encoder.layer.6.attention.self.value.weight', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder.encoder.layer.6.crossattention.self.key.bias', 'text_encoder.encoder.layer.6.crossattention.self.key.weight', 'text_encoder.encoder.layer.6.crossattention.self.query.bias', 'text_encoder.encoder.layer.6.crossattention.self.query.weight', 'text_encoder.encoder.layer.6.crossattention.self.value.bias', 'text_encoder.encoder.layer.6.crossattention.self.value.weight', 'text_encoder.encoder.layer.6.intermediate.dense.bias', 'text_encoder.encoder.layer.6.intermediate.dense.weight', 'text_encoder.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.output.dense.bias', 'text_encoder.encoder.layer.6.output.dense.weight', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.attention.output.dense.bias', 'text_encoder.encoder.layer.7.attention.output.dense.weight', 'text_encoder.encoder.layer.7.attention.self.key.bias', 'text_encoder.encoder.layer.7.attention.self.key.weight', 'text_encoder.encoder.layer.7.attention.self.query.bias', 'text_encoder.encoder.layer.7.attention.self.query.weight', 'text_encoder.encoder.layer.7.attention.self.value.bias', 'text_encoder.encoder.layer.7.attention.self.value.weight', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder.encoder.layer.7.crossattention.self.key.bias', 'text_encoder.encoder.layer.7.crossattention.self.key.weight', 'text_encoder.encoder.layer.7.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.crossattention.self.query.weight', 'text_encoder.encoder.layer.7.crossattention.self.value.bias', 'text_encoder.encoder.layer.7.crossattention.self.value.weight', 'text_encoder.encoder.layer.7.intermediate.dense.bias', 'text_encoder.encoder.layer.7.intermediate.dense.weight', 'text_encoder.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.output.dense.bias', 'text_encoder.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.attention.output.dense.bias', 'text_encoder.encoder.layer.8.attention.output.dense.weight', 'text_encoder.encoder.layer.8.attention.self.key.bias', 'text_encoder.encoder.layer.8.attention.self.key.weight', 'text_encoder.encoder.layer.8.attention.self.query.bias', 'text_encoder.encoder.layer.8.attention.self.query.weight', 'text_encoder.encoder.layer.8.attention.self.value.bias', 'text_encoder.encoder.layer.8.attention.self.value.weight', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder.encoder.layer.8.crossattention.self.key.bias', 'text_encoder.encoder.layer.8.crossattention.self.key.weight', 'text_encoder.encoder.layer.8.crossattention.self.query.bias', 'text_encoder.encoder.layer.8.crossattention.self.query.weight', 'text_encoder.encoder.layer.8.crossattention.self.value.bias', 'text_encoder.encoder.layer.8.crossattention.self.value.weight', 'text_encoder.encoder.layer.8.intermediate.dense.bias', 'text_encoder.encoder.layer.8.intermediate.dense.weight', 'text_encoder.encoder.layer.8.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.output.dense.bias', 'text_encoder.encoder.layer.8.output.dense.weight', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.attention.output.dense.bias', 'text_encoder.encoder.layer.9.attention.output.dense.weight', 'text_encoder.encoder.layer.9.attention.self.key.bias', 'text_encoder.encoder.layer.9.attention.self.key.weight', 'text_encoder.encoder.layer.9.attention.self.query.bias', 'text_encoder.encoder.layer.9.attention.self.query.weight', 'text_encoder.encoder.layer.9.attention.self.value.bias', 'text_encoder.encoder.layer.9.attention.self.value.weight', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder.encoder.layer.9.crossattention.self.key.bias', 'text_encoder.encoder.layer.9.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.crossattention.self.query.bias', 'text_encoder.encoder.layer.9.crossattention.self.query.weight', 'text_encoder.encoder.layer.9.crossattention.self.value.bias', 'text_encoder.encoder.layer.9.crossattention.self.value.weight', 'text_encoder.encoder.layer.9.intermediate.dense.bias', 'text_encoder.encoder.layer.9.intermediate.dense.weight', 'text_encoder.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.output.dense.bias', 'text_encoder.encoder.layer.9.output.dense.weight', 'text_proj.bias', 'text_proj.weight', 'vision_proj.bias', 'vision_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "BLIP Epoch 1:   2%|▏         | 9/500 [00:01<01:04,  7.58it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438f0f6aabb14ef9995d118a6c73d07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 1:  39%|███▊      | 193/500 [00:30<00:43,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 1: 100%|██████████| 500/500 [01:13<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 1, Average Loss: 1.3196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 2:  17%|█▋        | 84/500 [00:11<00:55,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 2: 100%|██████████| 500/500 [01:09<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 2, Average Loss: 0.7007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 3:  87%|████████▋ | 434/500 [00:59<00:07,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 3: 100%|██████████| 500/500 [01:09<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 3, Average Loss: 0.6958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 4:  71%|███████   | 356/500 [00:48<00:20,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 4: 100%|██████████| 500/500 [01:09<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 4, Average Loss: 0.6948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 5:  39%|███▉      | 194/500 [00:27<00:45,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 5: 100%|██████████| 500/500 [01:08<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 5, Average Loss: 0.6904\n",
      "BLIP model saved to blip_model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import BlipForImageTextRetrieval, BlipProcessor\n",
    "from torchvision import transforms\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset with error handling\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.bad_images = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[\"image_path\"][idx]\n",
    "        caption = self.data[\"caption\"][idx]\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, FileNotFoundError, Exception) as e:\n",
    "            print(f\"Warning: Skipping {img_path} due to error: {e}\")\n",
    "            self.bad_images.append(img_path)\n",
    "            dummy_image = torch.zeros(3, 224, 224)\n",
    "            text = self.processor.tokenizer(\"Invalid image\", return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "            return dummy_image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        text = self.processor.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "        return image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "# BLIP Model with Contrastive Loss\n",
    "class BLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BLIP, self).__init__()\n",
    "        self.blip = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(768, self.projection_dim)  # ViT output dim\n",
    "        self.text_proj = nn.Linear(768, self.projection_dim)    # Text encoder output dim\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Vision encoding\n",
    "        vision_outputs = self.blip.vision_model(pixel_values=images).last_hidden_state[:, 0, :]\n",
    "        vision_embeds = self.vision_proj(vision_outputs)\n",
    "        \n",
    "        # Text encoding\n",
    "        text_outputs = self.blip.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        text_embeds = self.text_proj(text_outputs)\n",
    "        \n",
    "        return vision_embeds, text_embeds\n",
    "\n",
    "def contrastive_loss(vision_embeds, text_embeds, temperature=0.07):\n",
    "    logits = torch.matmul(vision_embeds, text_embeds.T) / temperature\n",
    "    labels = torch.arange(len(vision_embeds)).to(vision_embeds.device)\n",
    "    loss_i = nn.CrossEntropyLoss()(logits, labels)\n",
    "    loss_t = nn.CrossEntropyLoss()(logits.T, labels)\n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "# Setup\n",
    "dataset = CocoDataset(\"coco_dataset.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)  # Reduced batch size\n",
    "\n",
    "device = torch.device(\"cuda:3\")  # Hardcoded to GPU 3 (adjust if needed)\n",
    "torch.cuda.empty_cache()  # Clear memory\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "blip_model = BLIP().to(device)\n",
    "optimizer = torch.optim.Adam(blip_model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(5):\n",
    "    blip_model.train()\n",
    "    total_loss = 0\n",
    "    for images, input_ids, attention_mask in tqdm(dataloader, desc=f\"BLIP Epoch {epoch+1}\"):\n",
    "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            vision_embeds, text_embeds = blip_model(images, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(vision_embeds, text_embeds)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"BLIP Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "    if dataset.bad_images:\n",
    "        print(f\"Bad images skipped in Epoch {epoch+1}: {list(set(dataset.bad_images))}\")\n",
    "\n",
    "torch.save(blip_model.state_dict(), \"blip_model.pth\")\n",
    "print(\"BLIP model saved to blip_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78852a2c-d13e-41d0-9f4d-fa827c5cce3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6233e721194c05adbe601b724aec89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492ecccdfbb64145a01cbe78169a81ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b32c3fd1bcd40eba23c4b97f9a9bbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1d9fa902574bcfac47ae8adb17a446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa9b2fa602c467ea5f9188b109dd51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4148dea7e63b4d75b2ab481b8547f941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33260c48fdb14324b2755dd100b2a218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 1:   2%|▏         | 12/500 [00:01<00:48, 10.15it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6381db13854a0bb36dd9e821f955c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 1:  83%|████████▎ | 415/500 [00:40<00:07, 10.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 1: 100%|██████████| 500/500 [00:48<00:00, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 1, Average Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 2:  61%|██████    | 305/500 [00:28<00:17, 11.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 2: 100%|██████████| 500/500 [00:46<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 2, Average Loss: 0.1915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 3:  55%|█████▌    | 277/500 [00:25<00:20, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 3: 100%|██████████| 500/500 [00:45<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 3, Average Loss: 0.1519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 4:  29%|██▊       | 143/500 [00:13<00:31, 11.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 4: 100%|██████████| 500/500 [00:46<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 4, Average Loss: 0.0693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 5:  80%|███████▉  | 398/500 [00:36<00:08, 11.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 5: 100%|██████████| 500/500 [00:45<00:00, 11.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 5, Average Loss: 0.0620\n",
      "ViLT model saved to vilt_model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import ViltProcessor, ViltModel\n",
    "from torchvision import transforms\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset with error handling\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "        self.bad_images = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[\"image_path\"][idx]\n",
    "        caption = self.data[\"caption\"][idx]\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, FileNotFoundError, Exception) as e:\n",
    "            print(f\"Warning: Skipping {img_path} due to error: {e}\")\n",
    "            self.bad_images.append(img_path)\n",
    "            dummy_image = torch.zeros(3, 224, 224)\n",
    "            text = self.processor.tokenizer(\"Invalid image\", return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "            return dummy_image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        text = self.processor.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "        return image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "# ViLT Model with Contrastive Loss\n",
    "class VILT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VILT, self).__init__()\n",
    "        self.vilt = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "        self.projection_dim = 512\n",
    "        self.proj = nn.Linear(768, self.projection_dim)  # ViLT output dim is 768\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # ViLT processes images and text together\n",
    "        outputs = self.vilt(pixel_values=images, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeds = self.proj(outputs.last_hidden_state[:, 0, :])  # CLS token\n",
    "        # For contrastive loss, use same embeddings for vision and text (simplified)\n",
    "        return embeds, embeds\n",
    "\n",
    "def contrastive_loss(vision_embeds, text_embeds, temperature=0.07):\n",
    "    logits = torch.matmul(vision_embeds, text_embeds.T) / temperature\n",
    "    labels = torch.arange(len(vision_embeds)).to(vision_embeds.device)\n",
    "    loss_i = nn.CrossEntropyLoss()(logits, labels)\n",
    "    loss_t = nn.CrossEntropyLoss()(logits.T, labels)\n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "# Setup\n",
    "dataset = CocoDataset(\"coco_dataset.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)  # Reduced batch size\n",
    "\n",
    "device = torch.device(\"cuda:3\")  # Hardcoded to GPU 3 (adjust if needed)\n",
    "torch.cuda.empty_cache()  # Clear memory\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "vilt_model = VILT().to(device)\n",
    "optimizer = torch.optim.Adam(vilt_model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(5):\n",
    "    vilt_model.train()\n",
    "    total_loss = 0\n",
    "    for images, input_ids, attention_mask in tqdm(dataloader, desc=f\"ViLT Epoch {epoch+1}\"):\n",
    "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            vision_embeds, text_embeds = vilt_model(images, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(vision_embeds, text_embeds)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"ViLT Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "    if dataset.bad_images:\n",
    "        print(f\"Bad images skipped in Epoch {epoch+1}: {list(set(dataset.bad_images))}\")\n",
    "\n",
    "torch.save(vilt_model.state_dict(), \"vilt_model.pth\")\n",
    "print(\"ViLT model saved to vilt_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be47bc7-441e-4492-b651-b00e7025f6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df2686-0202-4d3a-ac32-75343cc8dc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6f7bbe-44ae-4397-bd11-03903eccb791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27901f56-3c32-484e-b801-b8eda65cec44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061cb48d-f93b-4ee6-a78e-1eaf156e1dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b34d5f0-6d62-4219-bb52-30ffed7cb3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 1:  93%|█████████▎| 465/500 [00:32<00:02, 14.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 1: 100%|██████████| 500/500 [00:34<00:00, 14.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 1, Average Loss: 7.6916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 2:  45%|████▍     | 223/500 [00:15<00:18, 14.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 2: 100%|██████████| 500/500 [00:33<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 2, Average Loss: 2.9676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 3:  83%|████████▎ | 413/500 [00:27<00:05, 14.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 3: 100%|██████████| 500/500 [00:33<00:00, 14.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 3, Average Loss: 1.3232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 4:  87%|████████▋ | 433/500 [00:29<00:04, 15.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 4: 100%|██████████| 500/500 [00:33<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 4, Average Loss: 1.1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 5:  37%|███▋      | 185/500 [00:12<00:21, 14.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 5: 100%|██████████| 500/500 [00:33<00:00, 14.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Epoch 5, Average Loss: 1.4876\n",
      "CLIP model saved to clip_from_scratch.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import transforms\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Multi-Head Attention (simplified)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = [t.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) for t in qkv]\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.out(out)\n",
    "\n",
    "# Vision Transformer Encoder\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, d_model=256, num_heads=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, d_model))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)  # [B, num_patches, d_model]\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1) + self.pos_embed\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x = x + attn(norm1(x))\n",
    "            x = x + ff(norm2(x))\n",
    "        return x[:, 0]  # CLS token\n",
    "\n",
    "# Text Transformer Encoder\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, d_model=256, num_heads=8, num_layers=6, max_len=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids) + self.pos_embed\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x = x + attn(norm1(x)) * attention_mask.unsqueeze(-1)\n",
    "            x = x + ff(norm2(x)) * attention_mask.unsqueeze(-1)\n",
    "        return x[:, 0]  # CLS token\n",
    "\n",
    "# CLIP from Scratch\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = ViTEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(256, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(256, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_embeds = self.vision_proj(self.vision_encoder(images))\n",
    "        text_embeds = self.text_proj(self.text_encoder(input_ids, attention_mask))\n",
    "        return vision_embeds, text_embeds\n",
    "\n",
    "# Dataset\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        from transformers import DistilBertTokenizer\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.bad_images = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[\"image_path\"][idx]\n",
    "        caption = self.data[\"caption\"][idx]\n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, FileNotFoundError, Exception) as e:\n",
    "            print(f\"Warning: Skipping {img_path} due to error: {e}\")\n",
    "            self.bad_images.append(img_path)\n",
    "            dummy_image = torch.zeros(3, 224, 224)\n",
    "            text = self.tokenizer(\"Invalid image\", return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "            return dummy_image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        text = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "        return image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "def contrastive_loss(vision_embeds, text_embeds, temperature=0.07):\n",
    "    logits = torch.matmul(vision_embeds, text_embeds.T) / temperature\n",
    "    labels = torch.arange(len(vision_embeds)).to(vision_embeds.device)\n",
    "    loss_i = nn.CrossEntropyLoss()(logits, labels)\n",
    "    loss_t = nn.CrossEntropyLoss()(logits.T, labels)\n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "# Setup and Training\n",
    "dataset = CocoDataset(\"coco_dataset.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "device = torch.device(\"cuda:3\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "clip_model = CLIP().to(device)\n",
    "optimizer = torch.optim.Adam(clip_model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(5):  # More epochs may be needed\n",
    "    clip_model.train()\n",
    "    total_loss = 0\n",
    "    for images, input_ids, attention_mask in tqdm(dataloader, desc=f\"CLIP Epoch {epoch+1}\"):\n",
    "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            vision_embeds, text_embeds = clip_model(images, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(vision_embeds, text_embeds)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"CLIP Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "    if dataset.bad_images:\n",
    "        print(f\"Bad images skipped in Epoch {epoch+1}: {list(set(dataset.bad_images))}\")\n",
    "\n",
    "torch.save(clip_model.state_dict(), \"clip_from_scratch.pth\")\n",
    "print(\"CLIP model saved to clip_from_scratch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419ae286-159d-4514-b863-2dbfa827f74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 1:  85%|████████▌ | 425/500 [00:29<00:05, 14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 1: 100%|██████████| 500/500 [00:34<00:00, 14.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 1, Average Loss: 7.0565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 2:  61%|██████    | 303/500 [00:20<00:13, 14.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 2: 100%|██████████| 500/500 [00:33<00:00, 14.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 2, Average Loss: 1.8474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 3:  32%|███▏      | 161/500 [00:11<00:20, 16.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 3: 100%|██████████| 500/500 [00:34<00:00, 14.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 3, Average Loss: 1.5648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 4:  43%|████▎     | 217/500 [00:14<00:18, 14.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 4: 100%|██████████| 500/500 [00:33<00:00, 14.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 4, Average Loss: 1.3408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 5:  19%|█▉        | 95/500 [00:06<00:28, 14.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 5: 100%|██████████| 500/500 [00:33<00:00, 14.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP Epoch 5, Average Loss: 1.5527\n",
      "BLIP model saved to blip_from_scratch.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import transforms\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reuse MultiHeadAttention, ViTEncoder, TextEncoder from CLIP\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = [t.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) for t in qkv]\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.out(out)\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, d_model=256, num_heads=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, d_model))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1) + self.pos_embed\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x = x + attn(norm1(x))\n",
    "            x = x + ff(norm2(x))\n",
    "        return x[:, 0]\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, d_model=256, num_heads=8, num_layers=6, max_len=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids) + self.pos_embed\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x = x + attn(norm1(x)) * attention_mask.unsqueeze(-1)\n",
    "            x = x + ff(norm2(x)) * attention_mask.unsqueeze(-1)\n",
    "        return x[:, 0]\n",
    "\n",
    "# BLIP from Scratch\n",
    "class BLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = ViTEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(256, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(256, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_embeds = self.vision_proj(self.vision_encoder(images))\n",
    "        text_embeds = self.text_proj(self.text_encoder(input_ids, attention_mask))\n",
    "        return vision_embeds, text_embeds\n",
    "\n",
    "# Dataset\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        from transformers import DistilBertTokenizer\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.bad_images = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[\"image_path\"][idx]\n",
    "        caption = self.data[\"caption\"][idx]\n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, FileNotFoundError, Exception) as e:\n",
    "            print(f\"Warning: Skipping {img_path} due to error: {e}\")\n",
    "            self.bad_images.append(img_path)\n",
    "            dummy_image = torch.zeros(3, 224, 224)\n",
    "            text = self.tokenizer(\"Invalid image\", return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "            return dummy_image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        text = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "        return image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "def contrastive_loss(vision_embeds, text_embeds, temperature=0.07):\n",
    "    logits = torch.matmul(vision_embeds, text_embeds.T) / temperature\n",
    "    labels = torch.arange(len(vision_embeds)).to(vision_embeds.device)\n",
    "    loss_i = nn.CrossEntropyLoss()(logits, labels)\n",
    "    loss_t = nn.CrossEntropyLoss()(logits.T, labels)\n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "# Setup and Training\n",
    "dataset = CocoDataset(\"coco_dataset.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "device = torch.device(\"cuda:3\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "blip_model = BLIP().to(device)\n",
    "optimizer = torch.optim.Adam(blip_model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(5):  # More epochs may be needed\n",
    "    blip_model.train()\n",
    "    total_loss = 0\n",
    "    for images, input_ids, attention_mask in tqdm(dataloader, desc=f\"BLIP Epoch {epoch+1}\"):\n",
    "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            vision_embeds, text_embeds = blip_model(images, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(vision_embeds, text_embeds)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"BLIP Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "    if dataset.bad_images:\n",
    "        print(f\"Bad images skipped in Epoch {epoch+1}: {list(set(dataset.bad_images))}\")\n",
    "\n",
    "torch.save(blip_model.state_dict(), \"blip_from_scratch.pth\")\n",
    "print(\"BLIP model saved to blip_from_scratch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cd48d91-8de4-4a55-aca2-a800f85effac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 1:  89%|████████▊ | 443/500 [00:19<00:02, 23.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 1: 100%|██████████| 500/500 [00:22<00:00, 22.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 1, Average Loss: 26.7046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 2:  81%|████████  | 406/500 [00:18<00:04, 22.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 2: 100%|██████████| 500/500 [00:22<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 2, Average Loss: 6.8063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 3:  18%|█▊        | 91/500 [00:04<00:18, 22.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 3: 100%|██████████| 500/500 [00:22<00:00, 21.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 3, Average Loss: 19.1642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 4:  74%|███████▍  | 371/500 [00:16<00:05, 22.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 4: 100%|██████████| 500/500 [00:22<00:00, 22.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 4, Average Loss: 3.5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 5:  33%|███▎      | 166/500 [00:07<00:15, 21.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 5: 100%|██████████| 500/500 [00:22<00:00, 21.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViLT Epoch 5, Average Loss: 0.7040\n",
      "ViLT model saved to vilt_from_scratch.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import transforms\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = [t.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) for t in qkv]\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.out(out)\n",
    "\n",
    "# ViLT Encoder\n",
    "class ViltEncoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, vocab_size=30522, d_model=256, num_heads=8, num_layers=6, max_len=32):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        self.text_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + max_len + 1, d_model))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Image patches\n",
    "        img_embeds = self.patch_embed(images).flatten(2).transpose(1, 2)\n",
    "        # Text tokens\n",
    "        text_embeds = self.text_embed(input_ids)\n",
    "        # Concatenate\n",
    "        x = torch.cat([img_embeds, text_embeds], dim=1)\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1) + self.pos_embed\n",
    "        # Attention mask for images + text\n",
    "        img_mask = torch.ones(x.size(0), img_embeds.size(1) + 1, device=x.device)\n",
    "        full_mask = torch.cat([img_mask, attention_mask], dim=1)\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x = x + attn(norm1(x)) * full_mask.unsqueeze(-1)\n",
    "            x = x + ff(norm2(x)) * full_mask.unsqueeze(-1)\n",
    "        return x[:, 0]\n",
    "\n",
    "# ViLT from Scratch\n",
    "class VILT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vilt = ViltEncoder()\n",
    "        self.projection_dim = 512\n",
    "        self.proj = nn.Linear(256, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        embeds = self.proj(self.vilt(images, input_ids, attention_mask))\n",
    "        return embeds, embeds  # Same embeddings for contrastive loss\n",
    "\n",
    "# Dataset\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        from transformers import DistilBertTokenizer\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.bad_images = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[\"image_path\"][idx]\n",
    "        caption = self.data[\"caption\"][idx]\n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, FileNotFoundError, Exception) as e:\n",
    "            print(f\"Warning: Skipping {img_path} due to error: {e}\")\n",
    "            self.bad_images.append(img_path)\n",
    "            dummy_image = torch.zeros(3, 224, 224)\n",
    "            text = self.tokenizer(\"Invalid image\", return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "            return dummy_image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        text = self.tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "        return image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "def contrastive_loss(vision_embeds, text_embeds, temperature=0.07):\n",
    "    logits = torch.matmul(vision_embeds, text_embeds.T) / temperature\n",
    "    labels = torch.arange(len(vision_embeds)).to(vision_embeds.device)\n",
    "    loss_i = nn.CrossEntropyLoss()(logits, labels)\n",
    "    loss_t = nn.CrossEntropyLoss()(logits.T, labels)\n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "# Setup and Training\n",
    "dataset = CocoDataset(\"coco_dataset.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "device = torch.device(\"cuda:3\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "vilt_model = VILT().to(device)\n",
    "optimizer = torch.optim.Adam(vilt_model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(5):  # More epochs may be needed\n",
    "    vilt_model.train()\n",
    "    total_loss = 0\n",
    "    for images, input_ids, attention_mask in tqdm(dataloader, desc=f\"ViLT Epoch {epoch+1}\"):\n",
    "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            vision_embeds, text_embeds = vilt_model(images, input_ids, attention_mask)\n",
    "            loss = contrastive_loss(vision_embeds, text_embeds)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"ViLT Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "    if dataset.bad_images:\n",
    "        print(f\"Bad images skipped in Epoch {epoch+1}: {list(set(dataset.bad_images))}\")\n",
    "\n",
    "torch.save(vilt_model.state_dict(), \"vilt_from_scratch.pth\")\n",
    "print(\"ViLT model saved to vilt_from_scratch.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9259f4b-4b0a-4fb7-a1c9-f5bdd30c69db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c106bc-202f-4382-a843-a9d8f30fbd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f222b53f-b4c7-4bd3-8bc7-375a86582c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a340087c-be7c-42b0-84ec-139477a8ca48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading CLIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/deit-small-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating CLIP:  74%|███████▍  | 93/125 [00:03<00:00, 32.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CLIP: 100%|██████████| 125/125 [00:04<00:00, 29.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading BLIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipForImageTextRetrieval were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['itm_head.bias', 'itm_head.weight', 'text_encoder.embeddings.LayerNorm.bias', 'text_encoder.embeddings.LayerNorm.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.word_embeddings.weight', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.attention.output.dense.bias', 'text_encoder.encoder.layer.0.attention.output.dense.weight', 'text_encoder.encoder.layer.0.attention.self.key.bias', 'text_encoder.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.0.attention.self.query.weight', 'text_encoder.encoder.layer.0.attention.self.value.bias', 'text_encoder.encoder.layer.0.attention.self.value.weight', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.crossattention.output.dense.bias', 'text_encoder.encoder.layer.0.crossattention.output.dense.weight', 'text_encoder.encoder.layer.0.crossattention.self.key.bias', 'text_encoder.encoder.layer.0.crossattention.self.key.weight', 'text_encoder.encoder.layer.0.crossattention.self.query.bias', 'text_encoder.encoder.layer.0.crossattention.self.query.weight', 'text_encoder.encoder.layer.0.crossattention.self.value.bias', 'text_encoder.encoder.layer.0.crossattention.self.value.weight', 'text_encoder.encoder.layer.0.intermediate.dense.bias', 'text_encoder.encoder.layer.0.intermediate.dense.weight', 'text_encoder.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.output.dense.bias', 'text_encoder.encoder.layer.0.output.dense.weight', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.attention.output.dense.bias', 'text_encoder.encoder.layer.1.attention.output.dense.weight', 'text_encoder.encoder.layer.1.attention.self.key.bias', 'text_encoder.encoder.layer.1.attention.self.key.weight', 'text_encoder.encoder.layer.1.attention.self.query.bias', 'text_encoder.encoder.layer.1.attention.self.query.weight', 'text_encoder.encoder.layer.1.attention.self.value.bias', 'text_encoder.encoder.layer.1.attention.self.value.weight', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder.encoder.layer.1.crossattention.output.dense.weight', 'text_encoder.encoder.layer.1.crossattention.self.key.bias', 'text_encoder.encoder.layer.1.crossattention.self.key.weight', 'text_encoder.encoder.layer.1.crossattention.self.query.bias', 'text_encoder.encoder.layer.1.crossattention.self.query.weight', 'text_encoder.encoder.layer.1.crossattention.self.value.bias', 'text_encoder.encoder.layer.1.crossattention.self.value.weight', 'text_encoder.encoder.layer.1.intermediate.dense.bias', 'text_encoder.encoder.layer.1.intermediate.dense.weight', 'text_encoder.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.output.dense.bias', 'text_encoder.encoder.layer.1.output.dense.weight', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.attention.output.dense.bias', 'text_encoder.encoder.layer.10.attention.output.dense.weight', 'text_encoder.encoder.layer.10.attention.self.key.bias', 'text_encoder.encoder.layer.10.attention.self.key.weight', 'text_encoder.encoder.layer.10.attention.self.query.bias', 'text_encoder.encoder.layer.10.attention.self.query.weight', 'text_encoder.encoder.layer.10.attention.self.value.bias', 'text_encoder.encoder.layer.10.attention.self.value.weight', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder.encoder.layer.10.crossattention.self.key.bias', 'text_encoder.encoder.layer.10.crossattention.self.key.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.bias', 'text_encoder.encoder.layer.10.crossattention.self.query.weight', 'text_encoder.encoder.layer.10.crossattention.self.value.bias', 'text_encoder.encoder.layer.10.crossattention.self.value.weight', 'text_encoder.encoder.layer.10.intermediate.dense.bias', 'text_encoder.encoder.layer.10.intermediate.dense.weight', 'text_encoder.encoder.layer.10.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.output.dense.bias', 'text_encoder.encoder.layer.10.output.dense.weight', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.output.dense.bias', 'text_encoder.encoder.layer.11.attention.output.dense.weight', 'text_encoder.encoder.layer.11.attention.self.key.bias', 'text_encoder.encoder.layer.11.attention.self.key.weight', 'text_encoder.encoder.layer.11.attention.self.query.bias', 'text_encoder.encoder.layer.11.attention.self.query.weight', 'text_encoder.encoder.layer.11.attention.self.value.bias', 'text_encoder.encoder.layer.11.attention.self.value.weight', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder.encoder.layer.11.crossattention.self.key.bias', 'text_encoder.encoder.layer.11.crossattention.self.key.weight', 'text_encoder.encoder.layer.11.crossattention.self.query.bias', 'text_encoder.encoder.layer.11.crossattention.self.query.weight', 'text_encoder.encoder.layer.11.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.crossattention.self.value.weight', 'text_encoder.encoder.layer.11.intermediate.dense.bias', 'text_encoder.encoder.layer.11.intermediate.dense.weight', 'text_encoder.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.output.dense.bias', 'text_encoder.encoder.layer.11.output.dense.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.output.dense.bias', 'text_encoder.encoder.layer.2.attention.output.dense.weight', 'text_encoder.encoder.layer.2.attention.self.key.bias', 'text_encoder.encoder.layer.2.attention.self.key.weight', 'text_encoder.encoder.layer.2.attention.self.query.bias', 'text_encoder.encoder.layer.2.attention.self.query.weight', 'text_encoder.encoder.layer.2.attention.self.value.bias', 'text_encoder.encoder.layer.2.attention.self.value.weight', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder.encoder.layer.2.crossattention.output.dense.weight', 'text_encoder.encoder.layer.2.crossattention.self.key.bias', 'text_encoder.encoder.layer.2.crossattention.self.key.weight', 'text_encoder.encoder.layer.2.crossattention.self.query.bias', 'text_encoder.encoder.layer.2.crossattention.self.query.weight', 'text_encoder.encoder.layer.2.crossattention.self.value.bias', 'text_encoder.encoder.layer.2.crossattention.self.value.weight', 'text_encoder.encoder.layer.2.intermediate.dense.bias', 'text_encoder.encoder.layer.2.intermediate.dense.weight', 'text_encoder.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.output.dense.bias', 'text_encoder.encoder.layer.2.output.dense.weight', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.attention.output.dense.bias', 'text_encoder.encoder.layer.3.attention.output.dense.weight', 'text_encoder.encoder.layer.3.attention.self.key.bias', 'text_encoder.encoder.layer.3.attention.self.key.weight', 'text_encoder.encoder.layer.3.attention.self.query.bias', 'text_encoder.encoder.layer.3.attention.self.query.weight', 'text_encoder.encoder.layer.3.attention.self.value.bias', 'text_encoder.encoder.layer.3.attention.self.value.weight', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.crossattention.output.dense.bias', 'text_encoder.encoder.layer.3.crossattention.output.dense.weight', 'text_encoder.encoder.layer.3.crossattention.self.key.bias', 'text_encoder.encoder.layer.3.crossattention.self.key.weight', 'text_encoder.encoder.layer.3.crossattention.self.query.bias', 'text_encoder.encoder.layer.3.crossattention.self.query.weight', 'text_encoder.encoder.layer.3.crossattention.self.value.bias', 'text_encoder.encoder.layer.3.crossattention.self.value.weight', 'text_encoder.encoder.layer.3.intermediate.dense.bias', 'text_encoder.encoder.layer.3.intermediate.dense.weight', 'text_encoder.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.output.dense.bias', 'text_encoder.encoder.layer.3.output.dense.weight', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.attention.output.dense.bias', 'text_encoder.encoder.layer.4.attention.output.dense.weight', 'text_encoder.encoder.layer.4.attention.self.key.bias', 'text_encoder.encoder.layer.4.attention.self.key.weight', 'text_encoder.encoder.layer.4.attention.self.query.bias', 'text_encoder.encoder.layer.4.attention.self.query.weight', 'text_encoder.encoder.layer.4.attention.self.value.bias', 'text_encoder.encoder.layer.4.attention.self.value.weight', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.crossattention.output.dense.bias', 'text_encoder.encoder.layer.4.crossattention.output.dense.weight', 'text_encoder.encoder.layer.4.crossattention.self.key.bias', 'text_encoder.encoder.layer.4.crossattention.self.key.weight', 'text_encoder.encoder.layer.4.crossattention.self.query.bias', 'text_encoder.encoder.layer.4.crossattention.self.query.weight', 'text_encoder.encoder.layer.4.crossattention.self.value.bias', 'text_encoder.encoder.layer.4.crossattention.self.value.weight', 'text_encoder.encoder.layer.4.intermediate.dense.bias', 'text_encoder.encoder.layer.4.intermediate.dense.weight', 'text_encoder.encoder.layer.4.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.4.output.dense.weight', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.attention.output.dense.bias', 'text_encoder.encoder.layer.5.attention.output.dense.weight', 'text_encoder.encoder.layer.5.attention.self.key.bias', 'text_encoder.encoder.layer.5.attention.self.key.weight', 'text_encoder.encoder.layer.5.attention.self.query.bias', 'text_encoder.encoder.layer.5.attention.self.query.weight', 'text_encoder.encoder.layer.5.attention.self.value.bias', 'text_encoder.encoder.layer.5.attention.self.value.weight', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder.encoder.layer.5.crossattention.output.dense.weight', 'text_encoder.encoder.layer.5.crossattention.self.key.bias', 'text_encoder.encoder.layer.5.crossattention.self.key.weight', 'text_encoder.encoder.layer.5.crossattention.self.query.bias', 'text_encoder.encoder.layer.5.crossattention.self.query.weight', 'text_encoder.encoder.layer.5.crossattention.self.value.bias', 'text_encoder.encoder.layer.5.crossattention.self.value.weight', 'text_encoder.encoder.layer.5.intermediate.dense.bias', 'text_encoder.encoder.layer.5.intermediate.dense.weight', 'text_encoder.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.output.dense.bias', 'text_encoder.encoder.layer.5.output.dense.weight', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.attention.output.dense.bias', 'text_encoder.encoder.layer.6.attention.output.dense.weight', 'text_encoder.encoder.layer.6.attention.self.key.bias', 'text_encoder.encoder.layer.6.attention.self.key.weight', 'text_encoder.encoder.layer.6.attention.self.query.bias', 'text_encoder.encoder.layer.6.attention.self.query.weight', 'text_encoder.encoder.layer.6.attention.self.value.bias', 'text_encoder.encoder.layer.6.attention.self.value.weight', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder.encoder.layer.6.crossattention.self.key.bias', 'text_encoder.encoder.layer.6.crossattention.self.key.weight', 'text_encoder.encoder.layer.6.crossattention.self.query.bias', 'text_encoder.encoder.layer.6.crossattention.self.query.weight', 'text_encoder.encoder.layer.6.crossattention.self.value.bias', 'text_encoder.encoder.layer.6.crossattention.self.value.weight', 'text_encoder.encoder.layer.6.intermediate.dense.bias', 'text_encoder.encoder.layer.6.intermediate.dense.weight', 'text_encoder.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.output.dense.bias', 'text_encoder.encoder.layer.6.output.dense.weight', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.attention.output.dense.bias', 'text_encoder.encoder.layer.7.attention.output.dense.weight', 'text_encoder.encoder.layer.7.attention.self.key.bias', 'text_encoder.encoder.layer.7.attention.self.key.weight', 'text_encoder.encoder.layer.7.attention.self.query.bias', 'text_encoder.encoder.layer.7.attention.self.query.weight', 'text_encoder.encoder.layer.7.attention.self.value.bias', 'text_encoder.encoder.layer.7.attention.self.value.weight', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder.encoder.layer.7.crossattention.self.key.bias', 'text_encoder.encoder.layer.7.crossattention.self.key.weight', 'text_encoder.encoder.layer.7.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.crossattention.self.query.weight', 'text_encoder.encoder.layer.7.crossattention.self.value.bias', 'text_encoder.encoder.layer.7.crossattention.self.value.weight', 'text_encoder.encoder.layer.7.intermediate.dense.bias', 'text_encoder.encoder.layer.7.intermediate.dense.weight', 'text_encoder.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.output.dense.bias', 'text_encoder.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.attention.output.dense.bias', 'text_encoder.encoder.layer.8.attention.output.dense.weight', 'text_encoder.encoder.layer.8.attention.self.key.bias', 'text_encoder.encoder.layer.8.attention.self.key.weight', 'text_encoder.encoder.layer.8.attention.self.query.bias', 'text_encoder.encoder.layer.8.attention.self.query.weight', 'text_encoder.encoder.layer.8.attention.self.value.bias', 'text_encoder.encoder.layer.8.attention.self.value.weight', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder.encoder.layer.8.crossattention.self.key.bias', 'text_encoder.encoder.layer.8.crossattention.self.key.weight', 'text_encoder.encoder.layer.8.crossattention.self.query.bias', 'text_encoder.encoder.layer.8.crossattention.self.query.weight', 'text_encoder.encoder.layer.8.crossattention.self.value.bias', 'text_encoder.encoder.layer.8.crossattention.self.value.weight', 'text_encoder.encoder.layer.8.intermediate.dense.bias', 'text_encoder.encoder.layer.8.intermediate.dense.weight', 'text_encoder.encoder.layer.8.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.output.dense.bias', 'text_encoder.encoder.layer.8.output.dense.weight', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.attention.output.dense.bias', 'text_encoder.encoder.layer.9.attention.output.dense.weight', 'text_encoder.encoder.layer.9.attention.self.key.bias', 'text_encoder.encoder.layer.9.attention.self.key.weight', 'text_encoder.encoder.layer.9.attention.self.query.bias', 'text_encoder.encoder.layer.9.attention.self.query.weight', 'text_encoder.encoder.layer.9.attention.self.value.bias', 'text_encoder.encoder.layer.9.attention.self.value.weight', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder.encoder.layer.9.crossattention.self.key.bias', 'text_encoder.encoder.layer.9.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.crossattention.self.query.bias', 'text_encoder.encoder.layer.9.crossattention.self.query.weight', 'text_encoder.encoder.layer.9.crossattention.self.value.bias', 'text_encoder.encoder.layer.9.crossattention.self.value.weight', 'text_encoder.encoder.layer.9.intermediate.dense.bias', 'text_encoder.encoder.layer.9.intermediate.dense.weight', 'text_encoder.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.output.dense.bias', 'text_encoder.encoder.layer.9.output.dense.weight', 'text_proj.bias', 'text_proj.weight', 'vision_proj.bias', 'vision_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating BLIP:  74%|███████▍  | 93/125 [00:06<00:02, 15.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLIP: 100%|██████████| 125/125 [00:08<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading VILT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating VILT:  75%|███████▌  | 94/125 [00:04<00:01, 25.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating VILT: 100%|██████████| 125/125 [00:05<00:00, 22.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CLIPFromScratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CLIPFromScratch:  77%|███████▋  | 96/125 [00:03<00:00, 31.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CLIPFromScratch: 100%|██████████| 125/125 [00:04<00:00, 29.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading BLIPFromScratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLIPFromScratch:  74%|███████▍  | 93/125 [00:03<00:01, 31.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLIPFromScratch: 100%|██████████| 125/125 [00:04<00:00, 28.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading VILTFromScratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating VILTFromScratch:  74%|███████▍  | 93/125 [00:03<00:01, 31.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating VILTFromScratch: 100%|██████████| 125/125 [00:04<00:00, 29.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison:\n",
      "--------------------------------------------------\n",
      "Model                Accuracy     Avg Similarity\n",
      "--------------------------------------------------\n",
      "CLIP                 0.1955       -0.0341     \n",
      "BLIP                 0.2125       0.1609      \n",
      "VILT                 1.0000       1.0000      \n",
      "CLIPFromScratch      0.1250       -0.1224     \n",
      "BLIPFromScratch      0.1255       0.0022      \n",
      "VILTFromScratch      1.0000       1.0000      \n",
      "\n",
      "Results saved to 'model_performance.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from transformers import ViTModel, DistilBertModel, DistilBertTokenizer, BlipForImageTextRetrieval, BlipProcessor, ViltProcessor, ViltModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Environment settings\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Dataset\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, model_type=\"clip\"):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.model_type = model_type\n",
    "        if model_type == \"blip\":\n",
    "            self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        elif model_type == \"vilt\":\n",
    "            self.processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "        else:  # clip or from-scratch\n",
    "            self.processor = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.bad_images = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[\"image_path\"][idx]\n",
    "        caption = self.data[\"caption\"][idx]\n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, FileNotFoundError, Exception) as e:\n",
    "            print(f\"Warning: Skipping {img_path} due to error: {e}\")\n",
    "            self.bad_images.append(img_path)\n",
    "            dummy_image = torch.zeros(3, 224, 224)\n",
    "            if self.model_type in [\"blip\", \"vilt\"]:\n",
    "                text = self.processor.tokenizer(\"Invalid image\", padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "            else:\n",
    "                text = self.processor(\"Invalid image\", padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "            return dummy_image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.model_type == \"blip\":\n",
    "            text = self.processor.tokenizer(caption, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        elif self.model_type == \"vilt\":\n",
    "            text = self.processor.tokenizer(caption, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        else:\n",
    "            text = self.processor(caption, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        return image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "# Pre-trained CLIP Model\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIP, self).__init__()\n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"facebook/deit-small-patch16-224\")\n",
    "        self.text_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(384, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(768, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_outputs = self.vision_encoder(images).last_hidden_state[:, 0, :]\n",
    "        text_outputs = self.text_encoder(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        vision_embeds = self.vision_proj(vision_outputs)\n",
    "        text_embeds = self.text_proj(text_outputs)\n",
    "        return vision_embeds, text_embeds\n",
    "\n",
    "# Pre-trained BLIP Model\n",
    "class BLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BLIP, self).__init__()\n",
    "        self.blip = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(768, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(768, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_outputs = self.blip.vision_model(pixel_values=images).last_hidden_state[:, 0, :]\n",
    "        text_outputs = self.blip.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        vision_embeds = self.vision_proj(vision_outputs)\n",
    "        text_embeds = self.text_proj(text_outputs)\n",
    "        return vision_embeds, text_embeds\n",
    "\n",
    "# Pre-trained ViLT Model\n",
    "class VILT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VILT, self).__init__()\n",
    "        self.vilt = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "        self.projection_dim = 512\n",
    "        self.proj = nn.Linear(768, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        outputs = self.vilt(pixel_values=images, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeds = self.proj(outputs.last_hidden_state[:, 0, :])\n",
    "        return embeds, embeds\n",
    "\n",
    "# From-Scratch Models\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = [t.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) for t in qkv]\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.out(out)\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, d_model=256, num_heads=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, d_model))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1) + self.pos_embed\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x = x + attn(norm1(x))\n",
    "            x = x + ff(norm2(x))\n",
    "        return x[:, 0]\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, d_model=256, num_heads=8, num_layers=6, max_len=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids) + self.pos_embed\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x = x + attn(norm1(x)) * attention_mask.unsqueeze(-1)\n",
    "            x = x + ff(norm2(x)) * attention_mask.unsqueeze(-1)\n",
    "        return x[:, 0]\n",
    "\n",
    "class CLIPFromScratch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = ViTEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(256, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(256, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_embeds = self.vision_proj(self.vision_encoder(images))\n",
    "        text_embeds = self.text_proj(self.text_encoder(input_ids, attention_mask))\n",
    "        return vision_embeds, text_embeds\n",
    "\n",
    "class BLIPFromScratch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = ViTEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(256, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(256, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_embeds = self.vision_proj(self.vision_encoder(images))\n",
    "        text_embeds = self.text_proj(self.text_encoder(input_ids, attention_mask))\n",
    "        return vision_embeds, text_embeds\n",
    "\n",
    "class ViltEncoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, vocab_size=30522, d_model=256, num_heads=8, num_layers=6, max_len=32):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        self.text_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + max_len + 1, d_model))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        img_embeds = self.patch_embed(images).flatten(2).transpose(1, 2)\n",
    "        text_embeds = self.text_embed(input_ids)\n",
    "        x = torch.cat([img_embeds, text_embeds], dim=1)\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1) + self.pos_embed\n",
    "        img_mask = torch.ones(x.size(0), img_embeds.size(1) + 1, device=x.device)\n",
    "        full_mask = torch.cat([img_mask, attention_mask], dim=1)\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x = x + attn(norm1(x)) * full_mask.unsqueeze(-1)\n",
    "            x = x + ff(norm2(x)) * full_mask.unsqueeze(-1)\n",
    "        return x[:, 0]\n",
    "\n",
    "class VILTFromScratch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vilt = ViltEncoder()\n",
    "        self.projection_dim = 512\n",
    "        self.proj = nn.Linear(256, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        embeds = self.proj(self.vilt(images, input_ids, attention_mask))\n",
    "        return embeds, embeds\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, device, model_name, model_type):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    similarities = []\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in tqdm(dataloader, desc=f\"Evaluating {model_name}\"):\n",
    "            images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "            vision_embeds, text_embeds = model(images, input_ids, attention_mask)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            vision_embeds = vision_embeds / vision_embeds.norm(dim=-1, keepdim=True)\n",
    "            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Compute cosine similarities\n",
    "            logits = torch.matmul(vision_embeds, text_embeds.T)\n",
    "            labels = torch.arange(len(images)).to(device)\n",
    "            \n",
    "            # Image-to-text retrieval (top-1 accuracy)\n",
    "            pred_i2t = logits.argmax(dim=1)\n",
    "            correct += (pred_i2t == labels).sum().item()\n",
    "            \n",
    "            # Text-to-image retrieval (top-1 accuracy)\n",
    "            pred_t2i = logits.T.argmax(dim=1)\n",
    "            correct += (pred_t2i == labels).sum().item()\n",
    "            \n",
    "            total += len(images) * 2  # Count both i2t and t2i\n",
    "            \n",
    "            # Average cosine similarity for matching pairs\n",
    "            sim = logits.diag().cpu().numpy()\n",
    "            similarities.extend(sim)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    return {\"model\": model_name, \"accuracy\": accuracy, \"avg_similarity\": avg_similarity}\n",
    "\n",
    "# Main evaluation\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Models and their configurations\n",
    "    models = [\n",
    "        (\"CLIP\", CLIP, \"clip_model.pth\", \"clip\"),\n",
    "        (\"BLIP\", BLIP, \"blip_model.pth\", \"blip\"),\n",
    "        (\"VILT\", VILT, \"vilt_model.pth\", \"vilt\"),\n",
    "        (\"CLIPFromScratch\", CLIPFromScratch, \"clip_from_scratch.pth\", \"clip\"),\n",
    "        (\"BLIPFromScratch\", BLIPFromScratch, \"blip_from_scratch.pth\", \"clip\"),\n",
    "        (\"VILTFromScratch\", VILTFromScratch, \"vilt_from_scratch.pth\", \"vilt\")\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for model_name, model_class, model_path, model_type in models:\n",
    "        print(f\"\\nLoading {model_name}...\")\n",
    "        # Load dataset\n",
    "        dataset = CocoDataset(\"coco_dataset.csv\", model_type=model_type)\n",
    "        dataloader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=2)  # Reduced batch size\n",
    "        \n",
    "        # Initialize and load model\n",
    "        model = model_class().to(device)\n",
    "        if os.path.exists(model_path):\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        else:\n",
    "            print(f\"Warning: {model_path} not found. Using untrained model.\")\n",
    "        \n",
    "        # Evaluate\n",
    "        result = evaluate_model(model, dataloader, device, model_name, model_type)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Log bad images\n",
    "        if dataset.bad_images:\n",
    "            print(f\"Bad images skipped in {model_name}: {list(set(dataset.bad_images))}\")\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Model':<20} {'Accuracy':<12} {'Avg Similarity':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    for result in results:\n",
    "        print(f\"{result['model']:<20} {result['accuracy']:<12.4f} {result['avg_similarity']:<12.4f}\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"model_performance.csv\", index=False)\n",
    "    print(\"\\nResults saved to 'model_performance.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000cfadf-5a3e-4b49-9dd7-1a8ba3c2b9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading CLIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/deit-small-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating CLIP:  74%|███████▎  | 92/125 [00:03<00:01, 30.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CLIP: 100%|██████████| 125/125 [00:04<00:00, 27.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading BLIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipForImageTextRetrieval were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['itm_head.bias', 'itm_head.weight', 'text_encoder.embeddings.LayerNorm.bias', 'text_encoder.embeddings.LayerNorm.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.word_embeddings.weight', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.attention.output.dense.bias', 'text_encoder.encoder.layer.0.attention.output.dense.weight', 'text_encoder.encoder.layer.0.attention.self.key.bias', 'text_encoder.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.0.attention.self.query.weight', 'text_encoder.encoder.layer.0.attention.self.value.bias', 'text_encoder.encoder.layer.0.attention.self.value.weight', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.crossattention.output.dense.bias', 'text_encoder.encoder.layer.0.crossattention.output.dense.weight', 'text_encoder.encoder.layer.0.crossattention.self.key.bias', 'text_encoder.encoder.layer.0.crossattention.self.key.weight', 'text_encoder.encoder.layer.0.crossattention.self.query.bias', 'text_encoder.encoder.layer.0.crossattention.self.query.weight', 'text_encoder.encoder.layer.0.crossattention.self.value.bias', 'text_encoder.encoder.layer.0.crossattention.self.value.weight', 'text_encoder.encoder.layer.0.intermediate.dense.bias', 'text_encoder.encoder.layer.0.intermediate.dense.weight', 'text_encoder.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.output.dense.bias', 'text_encoder.encoder.layer.0.output.dense.weight', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.attention.output.dense.bias', 'text_encoder.encoder.layer.1.attention.output.dense.weight', 'text_encoder.encoder.layer.1.attention.self.key.bias', 'text_encoder.encoder.layer.1.attention.self.key.weight', 'text_encoder.encoder.layer.1.attention.self.query.bias', 'text_encoder.encoder.layer.1.attention.self.query.weight', 'text_encoder.encoder.layer.1.attention.self.value.bias', 'text_encoder.encoder.layer.1.attention.self.value.weight', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder.encoder.layer.1.crossattention.output.dense.weight', 'text_encoder.encoder.layer.1.crossattention.self.key.bias', 'text_encoder.encoder.layer.1.crossattention.self.key.weight', 'text_encoder.encoder.layer.1.crossattention.self.query.bias', 'text_encoder.encoder.layer.1.crossattention.self.query.weight', 'text_encoder.encoder.layer.1.crossattention.self.value.bias', 'text_encoder.encoder.layer.1.crossattention.self.value.weight', 'text_encoder.encoder.layer.1.intermediate.dense.bias', 'text_encoder.encoder.layer.1.intermediate.dense.weight', 'text_encoder.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.output.dense.bias', 'text_encoder.encoder.layer.1.output.dense.weight', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.attention.output.dense.bias', 'text_encoder.encoder.layer.10.attention.output.dense.weight', 'text_encoder.encoder.layer.10.attention.self.key.bias', 'text_encoder.encoder.layer.10.attention.self.key.weight', 'text_encoder.encoder.layer.10.attention.self.query.bias', 'text_encoder.encoder.layer.10.attention.self.query.weight', 'text_encoder.encoder.layer.10.attention.self.value.bias', 'text_encoder.encoder.layer.10.attention.self.value.weight', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder.encoder.layer.10.crossattention.self.key.bias', 'text_encoder.encoder.layer.10.crossattention.self.key.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.bias', 'text_encoder.encoder.layer.10.crossattention.self.query.weight', 'text_encoder.encoder.layer.10.crossattention.self.value.bias', 'text_encoder.encoder.layer.10.crossattention.self.value.weight', 'text_encoder.encoder.layer.10.intermediate.dense.bias', 'text_encoder.encoder.layer.10.intermediate.dense.weight', 'text_encoder.encoder.layer.10.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.output.dense.bias', 'text_encoder.encoder.layer.10.output.dense.weight', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.output.dense.bias', 'text_encoder.encoder.layer.11.attention.output.dense.weight', 'text_encoder.encoder.layer.11.attention.self.key.bias', 'text_encoder.encoder.layer.11.attention.self.key.weight', 'text_encoder.encoder.layer.11.attention.self.query.bias', 'text_encoder.encoder.layer.11.attention.self.query.weight', 'text_encoder.encoder.layer.11.attention.self.value.bias', 'text_encoder.encoder.layer.11.attention.self.value.weight', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder.encoder.layer.11.crossattention.self.key.bias', 'text_encoder.encoder.layer.11.crossattention.self.key.weight', 'text_encoder.encoder.layer.11.crossattention.self.query.bias', 'text_encoder.encoder.layer.11.crossattention.self.query.weight', 'text_encoder.encoder.layer.11.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.crossattention.self.value.weight', 'text_encoder.encoder.layer.11.intermediate.dense.bias', 'text_encoder.encoder.layer.11.intermediate.dense.weight', 'text_encoder.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.output.dense.bias', 'text_encoder.encoder.layer.11.output.dense.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.output.dense.bias', 'text_encoder.encoder.layer.2.attention.output.dense.weight', 'text_encoder.encoder.layer.2.attention.self.key.bias', 'text_encoder.encoder.layer.2.attention.self.key.weight', 'text_encoder.encoder.layer.2.attention.self.query.bias', 'text_encoder.encoder.layer.2.attention.self.query.weight', 'text_encoder.encoder.layer.2.attention.self.value.bias', 'text_encoder.encoder.layer.2.attention.self.value.weight', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder.encoder.layer.2.crossattention.output.dense.weight', 'text_encoder.encoder.layer.2.crossattention.self.key.bias', 'text_encoder.encoder.layer.2.crossattention.self.key.weight', 'text_encoder.encoder.layer.2.crossattention.self.query.bias', 'text_encoder.encoder.layer.2.crossattention.self.query.weight', 'text_encoder.encoder.layer.2.crossattention.self.value.bias', 'text_encoder.encoder.layer.2.crossattention.self.value.weight', 'text_encoder.encoder.layer.2.intermediate.dense.bias', 'text_encoder.encoder.layer.2.intermediate.dense.weight', 'text_encoder.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.output.dense.bias', 'text_encoder.encoder.layer.2.output.dense.weight', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.attention.output.dense.bias', 'text_encoder.encoder.layer.3.attention.output.dense.weight', 'text_encoder.encoder.layer.3.attention.self.key.bias', 'text_encoder.encoder.layer.3.attention.self.key.weight', 'text_encoder.encoder.layer.3.attention.self.query.bias', 'text_encoder.encoder.layer.3.attention.self.query.weight', 'text_encoder.encoder.layer.3.attention.self.value.bias', 'text_encoder.encoder.layer.3.attention.self.value.weight', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.crossattention.output.dense.bias', 'text_encoder.encoder.layer.3.crossattention.output.dense.weight', 'text_encoder.encoder.layer.3.crossattention.self.key.bias', 'text_encoder.encoder.layer.3.crossattention.self.key.weight', 'text_encoder.encoder.layer.3.crossattention.self.query.bias', 'text_encoder.encoder.layer.3.crossattention.self.query.weight', 'text_encoder.encoder.layer.3.crossattention.self.value.bias', 'text_encoder.encoder.layer.3.crossattention.self.value.weight', 'text_encoder.encoder.layer.3.intermediate.dense.bias', 'text_encoder.encoder.layer.3.intermediate.dense.weight', 'text_encoder.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.output.dense.bias', 'text_encoder.encoder.layer.3.output.dense.weight', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.attention.output.dense.bias', 'text_encoder.encoder.layer.4.attention.output.dense.weight', 'text_encoder.encoder.layer.4.attention.self.key.bias', 'text_encoder.encoder.layer.4.attention.self.key.weight', 'text_encoder.encoder.layer.4.attention.self.query.bias', 'text_encoder.encoder.layer.4.attention.self.query.weight', 'text_encoder.encoder.layer.4.attention.self.value.bias', 'text_encoder.encoder.layer.4.attention.self.value.weight', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.crossattention.output.dense.bias', 'text_encoder.encoder.layer.4.crossattention.output.dense.weight', 'text_encoder.encoder.layer.4.crossattention.self.key.bias', 'text_encoder.encoder.layer.4.crossattention.self.key.weight', 'text_encoder.encoder.layer.4.crossattention.self.query.bias', 'text_encoder.encoder.layer.4.crossattention.self.query.weight', 'text_encoder.encoder.layer.4.crossattention.self.value.bias', 'text_encoder.encoder.layer.4.crossattention.self.value.weight', 'text_encoder.encoder.layer.4.intermediate.dense.bias', 'text_encoder.encoder.layer.4.intermediate.dense.weight', 'text_encoder.encoder.layer.4.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.4.output.dense.weight', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.attention.output.dense.bias', 'text_encoder.encoder.layer.5.attention.output.dense.weight', 'text_encoder.encoder.layer.5.attention.self.key.bias', 'text_encoder.encoder.layer.5.attention.self.key.weight', 'text_encoder.encoder.layer.5.attention.self.query.bias', 'text_encoder.encoder.layer.5.attention.self.query.weight', 'text_encoder.encoder.layer.5.attention.self.value.bias', 'text_encoder.encoder.layer.5.attention.self.value.weight', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder.encoder.layer.5.crossattention.output.dense.weight', 'text_encoder.encoder.layer.5.crossattention.self.key.bias', 'text_encoder.encoder.layer.5.crossattention.self.key.weight', 'text_encoder.encoder.layer.5.crossattention.self.query.bias', 'text_encoder.encoder.layer.5.crossattention.self.query.weight', 'text_encoder.encoder.layer.5.crossattention.self.value.bias', 'text_encoder.encoder.layer.5.crossattention.self.value.weight', 'text_encoder.encoder.layer.5.intermediate.dense.bias', 'text_encoder.encoder.layer.5.intermediate.dense.weight', 'text_encoder.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.output.dense.bias', 'text_encoder.encoder.layer.5.output.dense.weight', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.attention.output.dense.bias', 'text_encoder.encoder.layer.6.attention.output.dense.weight', 'text_encoder.encoder.layer.6.attention.self.key.bias', 'text_encoder.encoder.layer.6.attention.self.key.weight', 'text_encoder.encoder.layer.6.attention.self.query.bias', 'text_encoder.encoder.layer.6.attention.self.query.weight', 'text_encoder.encoder.layer.6.attention.self.value.bias', 'text_encoder.encoder.layer.6.attention.self.value.weight', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder.encoder.layer.6.crossattention.self.key.bias', 'text_encoder.encoder.layer.6.crossattention.self.key.weight', 'text_encoder.encoder.layer.6.crossattention.self.query.bias', 'text_encoder.encoder.layer.6.crossattention.self.query.weight', 'text_encoder.encoder.layer.6.crossattention.self.value.bias', 'text_encoder.encoder.layer.6.crossattention.self.value.weight', 'text_encoder.encoder.layer.6.intermediate.dense.bias', 'text_encoder.encoder.layer.6.intermediate.dense.weight', 'text_encoder.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.output.dense.bias', 'text_encoder.encoder.layer.6.output.dense.weight', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.attention.output.dense.bias', 'text_encoder.encoder.layer.7.attention.output.dense.weight', 'text_encoder.encoder.layer.7.attention.self.key.bias', 'text_encoder.encoder.layer.7.attention.self.key.weight', 'text_encoder.encoder.layer.7.attention.self.query.bias', 'text_encoder.encoder.layer.7.attention.self.query.weight', 'text_encoder.encoder.layer.7.attention.self.value.bias', 'text_encoder.encoder.layer.7.attention.self.value.weight', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder.encoder.layer.7.crossattention.self.key.bias', 'text_encoder.encoder.layer.7.crossattention.self.key.weight', 'text_encoder.encoder.layer.7.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.crossattention.self.query.weight', 'text_encoder.encoder.layer.7.crossattention.self.value.bias', 'text_encoder.encoder.layer.7.crossattention.self.value.weight', 'text_encoder.encoder.layer.7.intermediate.dense.bias', 'text_encoder.encoder.layer.7.intermediate.dense.weight', 'text_encoder.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.output.dense.bias', 'text_encoder.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.attention.output.dense.bias', 'text_encoder.encoder.layer.8.attention.output.dense.weight', 'text_encoder.encoder.layer.8.attention.self.key.bias', 'text_encoder.encoder.layer.8.attention.self.key.weight', 'text_encoder.encoder.layer.8.attention.self.query.bias', 'text_encoder.encoder.layer.8.attention.self.query.weight', 'text_encoder.encoder.layer.8.attention.self.value.bias', 'text_encoder.encoder.layer.8.attention.self.value.weight', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder.encoder.layer.8.crossattention.self.key.bias', 'text_encoder.encoder.layer.8.crossattention.self.key.weight', 'text_encoder.encoder.layer.8.crossattention.self.query.bias', 'text_encoder.encoder.layer.8.crossattention.self.query.weight', 'text_encoder.encoder.layer.8.crossattention.self.value.bias', 'text_encoder.encoder.layer.8.crossattention.self.value.weight', 'text_encoder.encoder.layer.8.intermediate.dense.bias', 'text_encoder.encoder.layer.8.intermediate.dense.weight', 'text_encoder.encoder.layer.8.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.output.dense.bias', 'text_encoder.encoder.layer.8.output.dense.weight', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.attention.output.dense.bias', 'text_encoder.encoder.layer.9.attention.output.dense.weight', 'text_encoder.encoder.layer.9.attention.self.key.bias', 'text_encoder.encoder.layer.9.attention.self.key.weight', 'text_encoder.encoder.layer.9.attention.self.query.bias', 'text_encoder.encoder.layer.9.attention.self.query.weight', 'text_encoder.encoder.layer.9.attention.self.value.bias', 'text_encoder.encoder.layer.9.attention.self.value.weight', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder.encoder.layer.9.crossattention.self.key.bias', 'text_encoder.encoder.layer.9.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.crossattention.self.query.bias', 'text_encoder.encoder.layer.9.crossattention.self.query.weight', 'text_encoder.encoder.layer.9.crossattention.self.value.bias', 'text_encoder.encoder.layer.9.crossattention.self.value.weight', 'text_encoder.encoder.layer.9.intermediate.dense.bias', 'text_encoder.encoder.layer.9.intermediate.dense.weight', 'text_encoder.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.output.dense.bias', 'text_encoder.encoder.layer.9.output.dense.weight', 'text_proj.bias', 'text_proj.weight', 'vision_proj.bias', 'vision_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Evaluating BLIP:  74%|███████▍  | 93/125 [00:06<00:02, 14.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLIP: 100%|██████████| 125/125 [00:08<00:00, 14.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading VILT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating VILT:  76%|███████▌  | 95/125 [00:04<00:01, 23.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating VILT: 100%|██████████| 125/125 [00:05<00:00, 21.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading CLIPFromScratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CLIPFromScratch:  74%|███████▍  | 93/125 [00:03<00:00, 33.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CLIPFromScratch: 100%|██████████| 125/125 [00:04<00:00, 31.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading BLIPFromScratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLIPFromScratch:  78%|███████▊  | 97/125 [00:03<00:00, 33.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BLIPFromScratch: 100%|██████████| 125/125 [00:04<00:00, 30.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading VILTFromScratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating VILTFromScratch:  78%|███████▊  | 97/125 [00:03<00:00, 33.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping coco_images/000000365426.jpg due to error: cannot identify image file 'coco_images/000000365426.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating VILTFromScratch: 100%|██████████| 125/125 [00:04<00:00, 30.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison:\n",
      "--------------------------------------------------------------------------------\n",
      "Model                Top-1 Err    Top-2 Err    Top-3 Err    Top-4 Err    Top-5 Err    Avg Sim     \n",
      "--------------------------------------------------------------------------------\n",
      "CLIP                 0.8045       0.6435       0.5015       0.3745       0.2600       -0.0341     \n",
      "BLIP                 0.7875       0.5995       0.4595       0.3210       0.2145       0.1609      \n",
      "VILT                 0.0000       0.0000       0.0000       0.0000       0.0000       1.0000      \n",
      "CLIPFromScratch      0.8750       0.7505       0.6245       0.5000       0.3715       -0.1224     \n",
      "BLIPFromScratch      0.8745       0.7480       0.6235       0.4975       0.3770       0.0022      \n",
      "VILTFromScratch      0.0000       0.0000       0.0000       0.0000       0.0000       1.0000      \n",
      "\n",
      "Results saved to 'model_performance_topk.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from transformers import ViTModel, DistilBertModel, DistilBertTokenizer, BlipForImageTextRetrieval, BlipProcessor, ViltProcessor, ViltModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Environment settings\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Dataset\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, model_type=\"clip\"):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.model_type = model_type\n",
    "        if model_type == \"blip\":\n",
    "            self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        elif model_type == \"vilt\":\n",
    "            self.processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "        else:  # clip or from-scratch\n",
    "            self.processor = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.bad_images = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[\"image_path\"][idx]\n",
    "        caption = self.data[\"caption\"][idx]\n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, FileNotFoundError, Exception) as e:\n",
    "            print(f\"Warning: Skipping {img_path} due to error: {e}\")\n",
    "            self.bad_images.append(img_path)\n",
    "            dummy_image = torch.zeros(3, 224, 224)\n",
    "            if self.model_type in [\"blip\", \"vilt\"]:\n",
    "                text = self.processor.tokenizer(\"Invalid image\", padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "            else:\n",
    "                text = self.processor(\"Invalid image\", padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "            return dummy_image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.model_type == \"blip\":\n",
    "            text = self.processor.tokenizer(caption, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        elif self.model_type == \"vilt\":\n",
    "            text = self.processor.tokenizer(caption, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        else:\n",
    "            text = self.processor(caption, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        return image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "# Pre-trained CLIP Model\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIP, self).__init__()\n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"facebook/deit-small-patch16-224\")\n",
    "        self.text_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(384, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(768, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_outputs = self.vision_encoder(images).last_hidden_state[:, 0, :]\n",
    "        text_outputs = self.text_encoder(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        vision_embeds = self.vision_proj(vision_outputs)\n",
    "        text_embeds = self.text_proj(text_outputs)\n",
    "        return vision_embeds, text_embeds\n",
    "\n",
    "# Pre-trained BLIP Model\n",
    "class BLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BLIP, self).__init__()\n",
    "        self.blip = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(768, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(768, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_outputs = self.blip.vision_model(pixel_values=images).last_hidden_state[:, 0, :]\n",
    "        text_outputs = self.blip.text_encoder(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        vision_embeds = self.vision_proj(vision_outputs)\n",
    "        text_embeds = self.text_proj(text_outputs)\n",
    "        return vision_embeds, text_embeds\n",
    "\n",
    "# Pre-trained ViLT Model\n",
    "class VILT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VILT, self).__init__()\n",
    "        self.vilt = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "        self.projection_dim = 512\n",
    "        self.proj = nn.Linear(768, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        outputs = self.vilt(pixel_values=images, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeds = self.proj(outputs.last_hidden_state[:, 0, :])\n",
    "        return embeds, embeds\n",
    "\n",
    "# From-Scratch Models\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = [t.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) for t in qkv]\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.out(out)\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, d_model=256, num_heads=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, d_model))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1) + self.pos_embed\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x = x + attn(norm1(x))\n",
    "            x = x + ff(norm2(x))\n",
    "        return x[:, 0]\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, d_model=256, num_heads=8, num_layers=6, max_len=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids) + self.pos_embed\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x = x + attn(norm1(x)) * attention_mask.unsqueeze(-1)\n",
    "            x = x + ff(norm2(x)) * attention_mask.unsqueeze(-1)\n",
    "        return x[:, 0]\n",
    "\n",
    "class CLIPFromScratch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = ViTEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(256, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(256, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_embeds = self.vision_proj(self.vision_encoder(images))\n",
    "        text_embeds = self.text_proj(self.text_encoder(input_ids, attention_mask))\n",
    "        return vision_embeds, text_embeds\n",
    "\n",
    "class BLIPFromScratch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = ViTEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(256, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(256, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_embeds = self.vision_proj(self.vision_encoder(images))\n",
    "        text_embeds = self.text_proj(self.text_encoder(input_ids, attention_mask))\n",
    "        return vision_embeds, text_embeds\n",
    "\n",
    "class ViltEncoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, vocab_size=30522, d_model=256, num_heads=8, num_layers=6, max_len=32):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        self.text_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + max_len + 1, d_model))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        img_embeds = self.patch_embed(images).flatten(2).transpose(1, 2)\n",
    "        text_embeds = self.text_embed(input_ids)\n",
    "        x = torch.cat([img_embeds, text_embeds], dim=1)\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1) + self.pos_embed\n",
    "        img_mask = torch.ones(x.size(0), img_embeds.size(1) + 1, device=x.device)\n",
    "        full_mask = torch.cat([img_mask, attention_mask], dim=1)\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x = x + attn(norm1(x)) * full_mask.unsqueeze(-1)\n",
    "            x = x + ff(norm2(x)) * full_mask.unsqueeze(-1)\n",
    "        return x[:, 0]\n",
    "\n",
    "class VILTFromScratch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vilt = ViltEncoder()\n",
    "        self.projection_dim = 512\n",
    "        self.proj = nn.Linear(256, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        embeds = self.proj(self.vilt(images, input_ids, attention_mask))\n",
    "        return embeds, embeds\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, dataloader, device, model_name, model_type):\n",
    "    model.eval()\n",
    "    topk_correct = {k: 0 for k in range(1, 6)}  # Track top-1 to top-5\n",
    "    total = 0\n",
    "    similarities = []\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in tqdm(dataloader, desc=f\"Evaluating {model_name}\"):\n",
    "            images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "            vision_embeds, text_embeds = model(images, input_ids, attention_mask)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            vision_embeds = vision_embeds / vision_embeds.norm(dim=-1, keepdim=True)\n",
    "            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Compute cosine similarities\n",
    "            logits = torch.matmul(vision_embeds, text_embeds.T)\n",
    "            labels = torch.arange(len(images)).to(device)\n",
    "            \n",
    "            # Top-k accuracy for i2t and t2i\n",
    "            for k in range(1, 6):\n",
    "                # Image-to-text\n",
    "                _, pred_i2t = logits.topk(k, dim=1)\n",
    "                correct_i2t = pred_i2t.eq(labels.view(-1, 1).expand_as(pred_i2t)).sum().item()\n",
    "                topk_correct[k] += correct_i2t\n",
    "                \n",
    "                # Text-to-image\n",
    "                _, pred_t2i = logits.T.topk(k, dim=1)\n",
    "                correct_t2i = pred_t2i.eq(labels.view(-1, 1).expand_as(pred_t2i)).sum().item()\n",
    "                topk_correct[k] += correct_t2i\n",
    "            \n",
    "            total += len(images) * 2  # Count both i2t and t2i\n",
    "            \n",
    "            # Average cosine similarity for matching pairs\n",
    "            sim = logits.diag().cpu().numpy()\n",
    "            similarities.extend(sim)\n",
    "    \n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"top1_accuracy\": topk_correct[1] / total,\n",
    "        \"top2_accuracy\": topk_correct[2] / total,\n",
    "        \"top3_accuracy\": topk_correct[3] / total,\n",
    "        \"top4_accuracy\": topk_correct[4] / total,\n",
    "        \"top5_accuracy\": topk_correct[5] / total,\n",
    "        \"top1_error\": 1 - (topk_correct[1] / total),\n",
    "        \"top2_error\": 1 - (topk_correct[2] / total),\n",
    "        \"top3_error\": 1 - (topk_correct[3] / total),\n",
    "        \"top4_error\": 1 - (topk_correct[4] / total),\n",
    "        \"top5_error\": 1 - (topk_correct[5] / total),\n",
    "        \"avg_similarity\": np.mean(similarities)\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Main evaluation\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Models and their configurations\n",
    "    models = [\n",
    "        (\"CLIP\", CLIP, \"clip_model.pth\", \"clip\"),\n",
    "        (\"BLIP\", BLIP, \"blip_model.pth\", \"blip\"),\n",
    "        (\"VILT\", VILT, \"vilt_model.pth\", \"vilt\"),\n",
    "        (\"CLIPFromScratch\", CLIPFromScratch, \"clip_from_scratch.pth\", \"clip\"),\n",
    "        (\"BLIPFromScratch\", BLIPFromScratch, \"blip_from_scratch.pth\", \"clip\"),\n",
    "        (\"VILTFromScratch\", VILTFromScratch, \"vilt_from_scratch.pth\", \"vilt\")\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for model_name, model_class, model_path, model_type in models:\n",
    "        print(f\"\\nLoading {model_name}...\")\n",
    "        # Load dataset\n",
    "        dataset = CocoDataset(\"coco_dataset.csv\", model_type=model_type)\n",
    "        dataloader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "        \n",
    "        # Initialize and load model\n",
    "        model = model_class().to(device)\n",
    "        if os.path.exists(model_path):\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        else:\n",
    "            print(f\"Warning: {model_path} not found. Using untrained model.\")\n",
    "        \n",
    "        # Evaluate\n",
    "        result = evaluate_model(model, dataloader, device, model_name, model_type)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Log bad images\n",
    "        if dataset.bad_images:\n",
    "            print(f\"Bad images skipped in {model_name}: {list(set(dataset.bad_images))}\")\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Model':<20} {'Top-1 Err':<12} {'Top-2 Err':<12} {'Top-3 Err':<12} {'Top-4 Err':<12} {'Top-5 Err':<12} {'Avg Sim':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    for result in results:\n",
    "        print(f\"{result['model']:<20} {result['top1_error']:<12.4f} {result['top2_error']:<12.4f} {result['top3_error']:<12.4f} {result['top4_error']:<12.4f} {result['top5_error']:<12.4f} {result['avg_similarity']:<12.4f}\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"model_performance_topk.csv\", index=False)\n",
    "    print(\"\\nResults saved to 'model_performance_topk.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd5647f-64f5-491a-8823-e01418cc523a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=12.68s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing metadata: 100%|██████████| 1000/1000 [00:00<00:00, 22419.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to 'coco_metadata.csv'\n",
      "                     image_path    category\n",
      "0  coco_images/000000391895.jpg  motorcycle\n",
      "1  coco_images/000000522418.jpg      person\n",
      "2  coco_images/000000184613.jpg         cow\n",
      "3  coco_images/000000318219.jpg      person\n",
      "4  coco_images/000000554625.jpg          tv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pycocotools.coco import COCO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load annotations\n",
    "annotations_dir = \"annotations\"\n",
    "instances_file = os.path.join(annotations_dir, \"annotations/instances_train2017.json\")\n",
    "coco = COCO(instances_file)\n",
    "\n",
    "# Load coco_dataset.csv to get the 1,000 images\n",
    "dataset_df = pd.read_csv(\"coco_dataset.csv\")\n",
    "image_paths = dataset_df[\"image_path\"].tolist()\n",
    "\n",
    "# Map image filenames to COCO image IDs\n",
    "filename_to_id = {img[\"file_name\"]: img[\"id\"] for img in coco.loadImgs(coco.getImgIds())}\n",
    "\n",
    "# Get categories for each image\n",
    "data = []\n",
    "for img_path in tqdm(image_paths, desc=\"Processing metadata\"):\n",
    "    filename = os.path.basename(img_path)\n",
    "    img_id = filename_to_id.get(filename)\n",
    "    if img_id is None:\n",
    "        print(f\"Warning: Image {filename} not found in annotations\")\n",
    "        category = \"unknown\"\n",
    "    else:\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        # Get the first category (simplified; you could aggregate multiple categories)\n",
    "        category_ids = [ann[\"category_id\"] for ann in anns]\n",
    "        if category_ids:\n",
    "            category = coco.loadCats(category_ids[0])[0][\"name\"]\n",
    "        else:\n",
    "            category = \"none\"\n",
    "    data.append({\"image_path\": img_path, \"category\": category})\n",
    "\n",
    "# Save to CSV\n",
    "metadata_df = pd.DataFrame(data)\n",
    "metadata_df.to_csv(\"coco_metadata.csv\", index=False)\n",
    "print(\"Metadata saved to 'coco_metadata.csv'\")\n",
    "print(metadata_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da7bb68-0aeb-4cfa-9d21-195693f47e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Processing CLIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/deit-small-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "noise 0.01: 100%|██████████| 125/125 [00:05<00:00, 23.55it/s]\n",
      "noise 0.05: 100%|██████████| 125/125 [00:05<00:00, 21.41it/s]\n",
      "noise 0.1: 100%|██████████| 125/125 [00:05<00:00, 23.93it/s]\n",
      "blur 1: 100%|██████████| 125/125 [00:06<00:00, 18.60it/s]\n",
      "blur 3: 100%|██████████| 125/125 [00:07<00:00, 17.55it/s]\n",
      "blur 5: 100%|██████████| 125/125 [00:07<00:00, 17.05it/s]\n",
      "occlusion 32: 100%|██████████| 125/125 [00:05<00:00, 23.45it/s]\n",
      "occlusion 64: 100%|██████████| 125/125 [00:04<00:00, 25.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP cls_attn shape: torch.Size([8, 196])\n",
      "CLIP cls_attn shape: torch.Size([8, 196])\n",
      "CLIP cls_attn shape: torch.Size([8, 196])\n",
      "CLIP cls_attn shape: torch.Size([8, 196])\n",
      "CLIP cls_attn shape: torch.Size([8, 196])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fairness CLIP: 100%|██████████| 125/125 [00:05<00:00, 22.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing BLIP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipForImageTextRetrieval were not initialized from the model checkpoint at Salesforce/blip-image-captioning-base and are newly initialized: ['itm_head.bias', 'itm_head.weight', 'text_encoder.embeddings.LayerNorm.bias', 'text_encoder.embeddings.LayerNorm.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.word_embeddings.weight', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.attention.output.dense.bias', 'text_encoder.encoder.layer.0.attention.output.dense.weight', 'text_encoder.encoder.layer.0.attention.self.key.bias', 'text_encoder.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.0.attention.self.query.weight', 'text_encoder.encoder.layer.0.attention.self.value.bias', 'text_encoder.encoder.layer.0.attention.self.value.weight', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.crossattention.output.dense.bias', 'text_encoder.encoder.layer.0.crossattention.output.dense.weight', 'text_encoder.encoder.layer.0.crossattention.self.key.bias', 'text_encoder.encoder.layer.0.crossattention.self.key.weight', 'text_encoder.encoder.layer.0.crossattention.self.query.bias', 'text_encoder.encoder.layer.0.crossattention.self.query.weight', 'text_encoder.encoder.layer.0.crossattention.self.value.bias', 'text_encoder.encoder.layer.0.crossattention.self.value.weight', 'text_encoder.encoder.layer.0.intermediate.dense.bias', 'text_encoder.encoder.layer.0.intermediate.dense.weight', 'text_encoder.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.output.dense.bias', 'text_encoder.encoder.layer.0.output.dense.weight', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.attention.output.dense.bias', 'text_encoder.encoder.layer.1.attention.output.dense.weight', 'text_encoder.encoder.layer.1.attention.self.key.bias', 'text_encoder.encoder.layer.1.attention.self.key.weight', 'text_encoder.encoder.layer.1.attention.self.query.bias', 'text_encoder.encoder.layer.1.attention.self.query.weight', 'text_encoder.encoder.layer.1.attention.self.value.bias', 'text_encoder.encoder.layer.1.attention.self.value.weight', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder.encoder.layer.1.crossattention.output.dense.weight', 'text_encoder.encoder.layer.1.crossattention.self.key.bias', 'text_encoder.encoder.layer.1.crossattention.self.key.weight', 'text_encoder.encoder.layer.1.crossattention.self.query.bias', 'text_encoder.encoder.layer.1.crossattention.self.query.weight', 'text_encoder.encoder.layer.1.crossattention.self.value.bias', 'text_encoder.encoder.layer.1.crossattention.self.value.weight', 'text_encoder.encoder.layer.1.intermediate.dense.bias', 'text_encoder.encoder.layer.1.intermediate.dense.weight', 'text_encoder.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.output.dense.bias', 'text_encoder.encoder.layer.1.output.dense.weight', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.attention.output.dense.bias', 'text_encoder.encoder.layer.10.attention.output.dense.weight', 'text_encoder.encoder.layer.10.attention.self.key.bias', 'text_encoder.encoder.layer.10.attention.self.key.weight', 'text_encoder.encoder.layer.10.attention.self.query.bias', 'text_encoder.encoder.layer.10.attention.self.query.weight', 'text_encoder.encoder.layer.10.attention.self.value.bias', 'text_encoder.encoder.layer.10.attention.self.value.weight', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder.encoder.layer.10.crossattention.self.key.bias', 'text_encoder.encoder.layer.10.crossattention.self.key.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.bias', 'text_encoder.encoder.layer.10.crossattention.self.query.weight', 'text_encoder.encoder.layer.10.crossattention.self.value.bias', 'text_encoder.encoder.layer.10.crossattention.self.value.weight', 'text_encoder.encoder.layer.10.intermediate.dense.bias', 'text_encoder.encoder.layer.10.intermediate.dense.weight', 'text_encoder.encoder.layer.10.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.output.dense.bias', 'text_encoder.encoder.layer.10.output.dense.weight', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.output.dense.bias', 'text_encoder.encoder.layer.11.attention.output.dense.weight', 'text_encoder.encoder.layer.11.attention.self.key.bias', 'text_encoder.encoder.layer.11.attention.self.key.weight', 'text_encoder.encoder.layer.11.attention.self.query.bias', 'text_encoder.encoder.layer.11.attention.self.query.weight', 'text_encoder.encoder.layer.11.attention.self.value.bias', 'text_encoder.encoder.layer.11.attention.self.value.weight', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder.encoder.layer.11.crossattention.self.key.bias', 'text_encoder.encoder.layer.11.crossattention.self.key.weight', 'text_encoder.encoder.layer.11.crossattention.self.query.bias', 'text_encoder.encoder.layer.11.crossattention.self.query.weight', 'text_encoder.encoder.layer.11.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.crossattention.self.value.weight', 'text_encoder.encoder.layer.11.intermediate.dense.bias', 'text_encoder.encoder.layer.11.intermediate.dense.weight', 'text_encoder.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.output.dense.bias', 'text_encoder.encoder.layer.11.output.dense.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.output.dense.bias', 'text_encoder.encoder.layer.2.attention.output.dense.weight', 'text_encoder.encoder.layer.2.attention.self.key.bias', 'text_encoder.encoder.layer.2.attention.self.key.weight', 'text_encoder.encoder.layer.2.attention.self.query.bias', 'text_encoder.encoder.layer.2.attention.self.query.weight', 'text_encoder.encoder.layer.2.attention.self.value.bias', 'text_encoder.encoder.layer.2.attention.self.value.weight', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder.encoder.layer.2.crossattention.output.dense.weight', 'text_encoder.encoder.layer.2.crossattention.self.key.bias', 'text_encoder.encoder.layer.2.crossattention.self.key.weight', 'text_encoder.encoder.layer.2.crossattention.self.query.bias', 'text_encoder.encoder.layer.2.crossattention.self.query.weight', 'text_encoder.encoder.layer.2.crossattention.self.value.bias', 'text_encoder.encoder.layer.2.crossattention.self.value.weight', 'text_encoder.encoder.layer.2.intermediate.dense.bias', 'text_encoder.encoder.layer.2.intermediate.dense.weight', 'text_encoder.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.output.dense.bias', 'text_encoder.encoder.layer.2.output.dense.weight', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.attention.output.dense.bias', 'text_encoder.encoder.layer.3.attention.output.dense.weight', 'text_encoder.encoder.layer.3.attention.self.key.bias', 'text_encoder.encoder.layer.3.attention.self.key.weight', 'text_encoder.encoder.layer.3.attention.self.query.bias', 'text_encoder.encoder.layer.3.attention.self.query.weight', 'text_encoder.encoder.layer.3.attention.self.value.bias', 'text_encoder.encoder.layer.3.attention.self.value.weight', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.crossattention.output.dense.bias', 'text_encoder.encoder.layer.3.crossattention.output.dense.weight', 'text_encoder.encoder.layer.3.crossattention.self.key.bias', 'text_encoder.encoder.layer.3.crossattention.self.key.weight', 'text_encoder.encoder.layer.3.crossattention.self.query.bias', 'text_encoder.encoder.layer.3.crossattention.self.query.weight', 'text_encoder.encoder.layer.3.crossattention.self.value.bias', 'text_encoder.encoder.layer.3.crossattention.self.value.weight', 'text_encoder.encoder.layer.3.intermediate.dense.bias', 'text_encoder.encoder.layer.3.intermediate.dense.weight', 'text_encoder.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.output.dense.bias', 'text_encoder.encoder.layer.3.output.dense.weight', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.attention.output.dense.bias', 'text_encoder.encoder.layer.4.attention.output.dense.weight', 'text_encoder.encoder.layer.4.attention.self.key.bias', 'text_encoder.encoder.layer.4.attention.self.key.weight', 'text_encoder.encoder.layer.4.attention.self.query.bias', 'text_encoder.encoder.layer.4.attention.self.query.weight', 'text_encoder.encoder.layer.4.attention.self.value.bias', 'text_encoder.encoder.layer.4.attention.self.value.weight', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.crossattention.output.dense.bias', 'text_encoder.encoder.layer.4.crossattention.output.dense.weight', 'text_encoder.encoder.layer.4.crossattention.self.key.bias', 'text_encoder.encoder.layer.4.crossattention.self.key.weight', 'text_encoder.encoder.layer.4.crossattention.self.query.bias', 'text_encoder.encoder.layer.4.crossattention.self.query.weight', 'text_encoder.encoder.layer.4.crossattention.self.value.bias', 'text_encoder.encoder.layer.4.crossattention.self.value.weight', 'text_encoder.encoder.layer.4.intermediate.dense.bias', 'text_encoder.encoder.layer.4.intermediate.dense.weight', 'text_encoder.encoder.layer.4.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.4.output.dense.weight', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.attention.output.dense.bias', 'text_encoder.encoder.layer.5.attention.output.dense.weight', 'text_encoder.encoder.layer.5.attention.self.key.bias', 'text_encoder.encoder.layer.5.attention.self.key.weight', 'text_encoder.encoder.layer.5.attention.self.query.bias', 'text_encoder.encoder.layer.5.attention.self.query.weight', 'text_encoder.encoder.layer.5.attention.self.value.bias', 'text_encoder.encoder.layer.5.attention.self.value.weight', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder.encoder.layer.5.crossattention.output.dense.weight', 'text_encoder.encoder.layer.5.crossattention.self.key.bias', 'text_encoder.encoder.layer.5.crossattention.self.key.weight', 'text_encoder.encoder.layer.5.crossattention.self.query.bias', 'text_encoder.encoder.layer.5.crossattention.self.query.weight', 'text_encoder.encoder.layer.5.crossattention.self.value.bias', 'text_encoder.encoder.layer.5.crossattention.self.value.weight', 'text_encoder.encoder.layer.5.intermediate.dense.bias', 'text_encoder.encoder.layer.5.intermediate.dense.weight', 'text_encoder.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.output.dense.bias', 'text_encoder.encoder.layer.5.output.dense.weight', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.attention.output.dense.bias', 'text_encoder.encoder.layer.6.attention.output.dense.weight', 'text_encoder.encoder.layer.6.attention.self.key.bias', 'text_encoder.encoder.layer.6.attention.self.key.weight', 'text_encoder.encoder.layer.6.attention.self.query.bias', 'text_encoder.encoder.layer.6.attention.self.query.weight', 'text_encoder.encoder.layer.6.attention.self.value.bias', 'text_encoder.encoder.layer.6.attention.self.value.weight', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder.encoder.layer.6.crossattention.self.key.bias', 'text_encoder.encoder.layer.6.crossattention.self.key.weight', 'text_encoder.encoder.layer.6.crossattention.self.query.bias', 'text_encoder.encoder.layer.6.crossattention.self.query.weight', 'text_encoder.encoder.layer.6.crossattention.self.value.bias', 'text_encoder.encoder.layer.6.crossattention.self.value.weight', 'text_encoder.encoder.layer.6.intermediate.dense.bias', 'text_encoder.encoder.layer.6.intermediate.dense.weight', 'text_encoder.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.output.dense.bias', 'text_encoder.encoder.layer.6.output.dense.weight', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.attention.output.dense.bias', 'text_encoder.encoder.layer.7.attention.output.dense.weight', 'text_encoder.encoder.layer.7.attention.self.key.bias', 'text_encoder.encoder.layer.7.attention.self.key.weight', 'text_encoder.encoder.layer.7.attention.self.query.bias', 'text_encoder.encoder.layer.7.attention.self.query.weight', 'text_encoder.encoder.layer.7.attention.self.value.bias', 'text_encoder.encoder.layer.7.attention.self.value.weight', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder.encoder.layer.7.crossattention.self.key.bias', 'text_encoder.encoder.layer.7.crossattention.self.key.weight', 'text_encoder.encoder.layer.7.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.crossattention.self.query.weight', 'text_encoder.encoder.layer.7.crossattention.self.value.bias', 'text_encoder.encoder.layer.7.crossattention.self.value.weight', 'text_encoder.encoder.layer.7.intermediate.dense.bias', 'text_encoder.encoder.layer.7.intermediate.dense.weight', 'text_encoder.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.output.dense.bias', 'text_encoder.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.attention.output.dense.bias', 'text_encoder.encoder.layer.8.attention.output.dense.weight', 'text_encoder.encoder.layer.8.attention.self.key.bias', 'text_encoder.encoder.layer.8.attention.self.key.weight', 'text_encoder.encoder.layer.8.attention.self.query.bias', 'text_encoder.encoder.layer.8.attention.self.query.weight', 'text_encoder.encoder.layer.8.attention.self.value.bias', 'text_encoder.encoder.layer.8.attention.self.value.weight', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder.encoder.layer.8.crossattention.self.key.bias', 'text_encoder.encoder.layer.8.crossattention.self.key.weight', 'text_encoder.encoder.layer.8.crossattention.self.query.bias', 'text_encoder.encoder.layer.8.crossattention.self.query.weight', 'text_encoder.encoder.layer.8.crossattention.self.value.bias', 'text_encoder.encoder.layer.8.crossattention.self.value.weight', 'text_encoder.encoder.layer.8.intermediate.dense.bias', 'text_encoder.encoder.layer.8.intermediate.dense.weight', 'text_encoder.encoder.layer.8.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.output.dense.bias', 'text_encoder.encoder.layer.8.output.dense.weight', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.attention.output.dense.bias', 'text_encoder.encoder.layer.9.attention.output.dense.weight', 'text_encoder.encoder.layer.9.attention.self.key.bias', 'text_encoder.encoder.layer.9.attention.self.key.weight', 'text_encoder.encoder.layer.9.attention.self.query.bias', 'text_encoder.encoder.layer.9.attention.self.query.weight', 'text_encoder.encoder.layer.9.attention.self.value.bias', 'text_encoder.encoder.layer.9.attention.self.value.weight', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder.encoder.layer.9.crossattention.self.key.bias', 'text_encoder.encoder.layer.9.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.crossattention.self.query.bias', 'text_encoder.encoder.layer.9.crossattention.self.query.weight', 'text_encoder.encoder.layer.9.crossattention.self.value.bias', 'text_encoder.encoder.layer.9.crossattention.self.value.weight', 'text_encoder.encoder.layer.9.intermediate.dense.bias', 'text_encoder.encoder.layer.9.intermediate.dense.weight', 'text_encoder.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.output.dense.bias', 'text_encoder.encoder.layer.9.output.dense.weight', 'text_proj.bias', 'text_proj.weight', 'vision_proj.bias', 'vision_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "noise 0.01: 100%|██████████| 125/125 [00:08<00:00, 14.46it/s]\n",
      "noise 0.05: 100%|██████████| 125/125 [00:08<00:00, 14.22it/s]\n",
      "noise 0.1: 100%|██████████| 125/125 [00:08<00:00, 14.50it/s]\n",
      "blur 1: 100%|██████████| 125/125 [00:09<00:00, 12.69it/s]\n",
      "blur 3: 100%|██████████| 125/125 [00:09<00:00, 12.89it/s]\n",
      "blur 5: 100%|██████████| 125/125 [00:09<00:00, 12.80it/s]\n",
      "occlusion 32: 100%|██████████| 125/125 [00:08<00:00, 14.33it/s]\n",
      "occlusion 64: 100%|██████████| 125/125 [00:08<00:00, 14.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP cls_attn shape: torch.Size([8, 196])\n",
      "BLIP cls_attn shape: torch.Size([8, 196])\n",
      "BLIP cls_attn shape: torch.Size([8, 196])\n",
      "BLIP cls_attn shape: torch.Size([8, 196])\n",
      "BLIP cls_attn shape: torch.Size([8, 196])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fairness BLIP: 100%|██████████| 125/125 [00:08<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing VILT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "noise 0.01: 100%|██████████| 125/125 [00:05<00:00, 21.32it/s]\n",
      "noise 0.05: 100%|██████████| 125/125 [00:05<00:00, 21.62it/s]\n",
      "noise 0.1: 100%|██████████| 125/125 [00:06<00:00, 20.74it/s]\n",
      "blur 1: 100%|██████████| 125/125 [00:07<00:00, 15.83it/s]\n",
      "blur 3: 100%|██████████| 125/125 [00:07<00:00, 16.95it/s]\n",
      "blur 5: 100%|██████████| 125/125 [00:07<00:00, 16.72it/s]\n",
      "occlusion 32: 100%|██████████| 125/125 [00:05<00:00, 21.50it/s]\n",
      "occlusion 64: 100%|██████████| 125/125 [00:05<00:00, 21.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VILT cls_attn shape: torch.Size([8, 81])\n",
      "Warning: Expected 196 patches, got 81. Skipping visualization for VILT.\n",
      "VILT cls_attn shape: torch.Size([8, 81])\n",
      "Warning: Expected 196 patches, got 81. Skipping visualization for VILT.\n",
      "VILT cls_attn shape: torch.Size([8, 81])\n",
      "Warning: Expected 196 patches, got 81. Skipping visualization for VILT.\n",
      "VILT cls_attn shape: torch.Size([8, 81])\n",
      "Warning: Expected 196 patches, got 81. Skipping visualization for VILT.\n",
      "VILT cls_attn shape: torch.Size([8, 81])\n",
      "Warning: Expected 196 patches, got 81. Skipping visualization for VILT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fairness VILT: 100%|██████████| 125/125 [00:05<00:00, 21.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing CLIPFromScratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "noise 0.01: 100%|██████████| 125/125 [00:04<00:00, 27.92it/s]\n",
      "noise 0.05: 100%|██████████| 125/125 [00:04<00:00, 26.18it/s]\n",
      "noise 0.1: 100%|██████████| 125/125 [00:04<00:00, 26.95it/s]\n",
      "blur 1: 100%|██████████| 125/125 [00:05<00:00, 23.24it/s]\n",
      "blur 3: 100%|██████████| 125/125 [00:05<00:00, 23.78it/s]\n",
      "blur 5: 100%|██████████| 125/125 [00:05<00:00, 22.43it/s]\n",
      "occlusion 32: 100%|██████████| 125/125 [00:04<00:00, 28.30it/s]\n",
      "occlusion 64: 100%|██████████| 125/125 [00:04<00:00, 28.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPFromScratch cls_attn shape: torch.Size([8, 196])\n",
      "CLIPFromScratch cls_attn shape: torch.Size([8, 196])\n",
      "CLIPFromScratch cls_attn shape: torch.Size([8, 196])\n",
      "CLIPFromScratch cls_attn shape: torch.Size([8, 196])\n",
      "CLIPFromScratch cls_attn shape: torch.Size([8, 196])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fairness CLIPFromScratch: 100%|██████████| 125/125 [00:04<00:00, 28.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing BLIPFromScratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "noise 0.01: 100%|██████████| 125/125 [00:04<00:00, 27.51it/s]\n",
      "noise 0.05: 100%|██████████| 125/125 [00:04<00:00, 26.90it/s]\n",
      "noise 0.1: 100%|██████████| 125/125 [00:04<00:00, 26.56it/s]\n",
      "blur 1: 100%|██████████| 125/125 [00:05<00:00, 23.28it/s]\n",
      "blur 3: 100%|██████████| 125/125 [00:05<00:00, 22.36it/s]\n",
      "blur 5: 100%|██████████| 125/125 [00:05<00:00, 21.69it/s]\n",
      "occlusion 32: 100%|██████████| 125/125 [00:04<00:00, 26.72it/s]\n",
      "occlusion 64: 100%|██████████| 125/125 [00:04<00:00, 27.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIPFromScratch cls_attn shape: torch.Size([8, 196])\n",
      "BLIPFromScratch cls_attn shape: torch.Size([8, 196])\n",
      "BLIPFromScratch cls_attn shape: torch.Size([8, 196])\n",
      "BLIPFromScratch cls_attn shape: torch.Size([8, 196])\n",
      "BLIPFromScratch cls_attn shape: torch.Size([8, 196])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fairness BLIPFromScratch: 100%|██████████| 125/125 [00:04<00:00, 27.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing VILTFromScratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "noise 0.01: 100%|██████████| 125/125 [00:04<00:00, 28.51it/s]\n",
      "noise 0.05: 100%|██████████| 125/125 [00:04<00:00, 27.83it/s]\n",
      "noise 0.1: 100%|██████████| 125/125 [00:04<00:00, 27.85it/s]\n",
      "blur 1: 100%|██████████| 125/125 [00:05<00:00, 23.65it/s]\n",
      "blur 3: 100%|██████████| 125/125 [00:05<00:00, 23.25it/s]\n",
      "blur 5: 100%|██████████| 125/125 [00:05<00:00, 22.18it/s]\n",
      "occlusion 32: 100%|██████████| 125/125 [00:04<00:00, 27.74it/s]\n",
      "occlusion 64: 100%|██████████| 125/125 [00:04<00:00, 28.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VILTFromScratch cls_attn shape: torch.Size([8, 196])\n",
      "VILTFromScratch cls_attn shape: torch.Size([8, 196])\n",
      "VILTFromScratch cls_attn shape: torch.Size([8, 196])\n",
      "VILTFromScratch cls_attn shape: torch.Size([8, 196])\n",
      "VILTFromScratch cls_attn shape: torch.Size([8, 196])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fairness VILTFromScratch: 100%|██████████| 125/125 [00:04<00:00, 29.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to 'robustness_interpretability_fairness_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "import cv2\n",
    "from transformers import ViTModel, DistilBertModel, DistilBertTokenizer, BlipForImageTextRetrieval, BlipProcessor, ViltProcessor, ViltModel\n",
    "\n",
    "# Environment settings\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Dataset\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, model_type=\"clip\"):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.model_type = model_type\n",
    "        if model_type == \"blip\":\n",
    "            self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        elif model_type == \"vilt\":\n",
    "            self.processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")\n",
    "        else:  # clip or from-scratch\n",
    "            self.processor = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.bad_images = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[\"image_path\"][idx]\n",
    "        caption = self.data[\"caption\"][idx]\n",
    "        try:\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except (UnidentifiedImageError, FileNotFoundError, Exception) as e:\n",
    "            print(f\"Warning: Skipping {img_path} due to error: {e}\")\n",
    "            self.bad_images.append(img_path)\n",
    "            dummy_image = torch.zeros(3, 224, 224)\n",
    "            if self.model_type in [\"blip\", \"vilt\"]:\n",
    "                text = self.processor.tokenizer(\"Invalid image\", padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "            else:\n",
    "                text = self.processor(\"Invalid image\", padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "            return dummy_image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.model_type == \"blip\":\n",
    "            text = self.processor.tokenizer(caption, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        elif self.model_type == \"vilt\":\n",
    "            text = self.processor.tokenizer(caption, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        else:\n",
    "            text = self.processor(caption, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
    "        return image, text[\"input_ids\"].squeeze(), text[\"attention_mask\"].squeeze()\n",
    "\n",
    "# Pre-trained Models\n",
    "class CLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIP, self).__init__()\n",
    "        self.vision_encoder = ViTModel.from_pretrained(\"facebook/deit-small-patch16-224\", output_attentions=True)\n",
    "        self.text_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(384, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(768, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_outputs = self.vision_encoder(images)\n",
    "        vision_embeds = self.vision_proj(vision_outputs.last_hidden_state[:, 0, :])\n",
    "        text_outputs = self.text_encoder(input_ids, attention_mask=attention_mask)\n",
    "        text_embeds = self.text_proj(text_outputs.last_hidden_state[:, 0, :])\n",
    "        return vision_embeds, text_embeds, vision_outputs.attentions\n",
    "\n",
    "class BLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BLIP, self).__init__()\n",
    "        self.blip = BlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.blip.vision_model.config.output_attentions = True\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(768, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(768, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_outputs = self.blip.vision_model(pixel_values=images)\n",
    "        vision_embeds = self.vision_proj(vision_outputs.last_hidden_state[:, 0, :])\n",
    "        text_outputs = self.blip.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeds = self.text_proj(text_outputs.last_hidden_state[:, 0, :])\n",
    "        return vision_embeds, text_embeds, vision_outputs.attentions\n",
    "\n",
    "class VILT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VILT, self).__init__()\n",
    "        self.vilt = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\", output_attentions=True)\n",
    "        self.projection_dim = 512\n",
    "        self.proj = nn.Linear(768, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        outputs = self.vilt(pixel_values=images, input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeds = self.proj(outputs.last_hidden_state[:, 0, :])\n",
    "        return embeds, embeds, outputs.attentions\n",
    "\n",
    "# From-Scratch Models\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = [t.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) for t in qkv]\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return out, attn\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, d_model=256, num_heads=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, d_model))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x).flatten(2).transpose(1, 2)\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1) + self.pos_embed\n",
    "        attentions = []\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x_norm = norm1(x)\n",
    "            attn_output, attn_weights = attn(x_norm)\n",
    "            x = x + attn_output\n",
    "            x = x + ff(norm2(x))\n",
    "            attentions.append(attn_weights)\n",
    "        return x[:, 0], attentions\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size=30522, d_model=256, num_heads=8, num_layers=6, max_len=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.embedding(input_ids) + self.pos_embed\n",
    "        attentions = []\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x_norm = norm1(x)\n",
    "            attn_output, attn_weights = attn(x_norm)\n",
    "            x = x + attn_output * attention_mask.unsqueeze(-1)\n",
    "            x = x + ff(norm2(x)) * attention_mask.unsqueeze(-1)\n",
    "            attentions.append(attn_weights)\n",
    "        return x[:, 0], attentions\n",
    "\n",
    "class CLIPFromScratch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = ViTEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(256, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(256, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_embeds, vision_attentions = self.vision_encoder(images)\n",
    "        text_embeds, text_attentions = self.text_encoder(input_ids, attention_mask)\n",
    "        vision_embeds = self.vision_proj(vision_embeds)\n",
    "        text_embeds = self.text_proj(text_embeds)\n",
    "        return vision_embeds, text_embeds, vision_attentions\n",
    "\n",
    "class BLIPFromScratch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vision_encoder = ViTEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.projection_dim = 512\n",
    "        self.vision_proj = nn.Linear(256, self.projection_dim)\n",
    "        self.text_proj = nn.Linear(256, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        vision_embeds, vision_attentions = self.vision_encoder(images)\n",
    "        text_embeds, text_attentions = self.text_encoder(input_ids, attention_mask)\n",
    "        vision_embeds = self.vision_proj(vision_embeds)\n",
    "        text_embeds = self.text_proj(text_embeds)\n",
    "        return vision_embeds, text_embeds, vision_attentions\n",
    "\n",
    "class ViltEncoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, vocab_size=30522, d_model=256, num_heads=8, num_layers=6, max_len=32):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "        self.text_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + max_len + 1, d_model))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(d_model),\n",
    "                MultiHeadAttention(d_model, num_heads),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Sequential(nn.Linear(d_model, d_model * 4), nn.GELU(), nn.Linear(d_model * 4, d_model))\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        img_embeds = self.patch_embed(images).flatten(2).transpose(1, 2)\n",
    "        text_embeds = self.text_embed(input_ids)\n",
    "        x = torch.cat([img_embeds, text_embeds], dim=1)\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1) + self.pos_embed\n",
    "        img_mask = torch.ones(x.size(0), img_embeds.size(1) + 1, device=x.device)\n",
    "        full_mask = torch.cat([img_mask, attention_mask], dim=1)\n",
    "        attentions = []\n",
    "        for norm1, attn, norm2, ff in self.layers:\n",
    "            x_norm = norm1(x)\n",
    "            attn_output, attn_weights = attn(x_norm)\n",
    "            x = x + attn_output * full_mask.unsqueeze(-1)\n",
    "            x = x + ff(norm2(x)) * full_mask.unsqueeze(-1)\n",
    "            attentions.append(attn_weights)\n",
    "        return x[:, 0], attentions\n",
    "\n",
    "class VILTFromScratch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vilt = ViltEncoder()\n",
    "        self.projection_dim = 512\n",
    "        self.proj = nn.Linear(256, self.projection_dim)\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        embeds, attentions = self.vilt(images, input_ids, attention_mask)\n",
    "        embeds = self.proj(embeds)\n",
    "        return embeds, embeds, attentions\n",
    "\n",
    "# Robustness Testing\n",
    "def robustness_test(model, dataloader, device, model_name, model_type, noise_levels=[0.01, 0.05, 0.1], blur_levels=[1, 3, 5]):\n",
    "    model.eval()\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"noise_results\": {},\n",
    "        \"blur_results\": {},\n",
    "        \"occlusion_results\": {}\n",
    "    }\n",
    "    \n",
    "    def add_gaussian_noise(image, std):\n",
    "        noise = torch.normal(mean=0.0, std=std, size=image.shape, device=image.device)\n",
    "        return torch.clamp(image + noise, 0, 1)\n",
    "    \n",
    "    def add_gaussian_blur(image, kernel_size):\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "        blurred = cv2.GaussianBlur(image_np, (kernel_size, kernel_size), 0)\n",
    "        return torch.tensor(blurred).permute(2, 0, 1).to(image.device)\n",
    "    \n",
    "    def add_occlusion(image, patch_size=32):\n",
    "        _, h, w = image.shape\n",
    "        x = np.random.randint(0, w - patch_size)\n",
    "        y = np.random.randint(0, h - patch_size)\n",
    "        image[:, y:y+patch_size, x:x+patch_size] = 0\n",
    "        return image\n",
    "    \n",
    "    def evaluate_perturbed(dataloader, perturbation_fn, levels, perturbation_type):\n",
    "        level_results = {}\n",
    "        for level in levels:\n",
    "            top1_correct = 0\n",
    "            total = 0\n",
    "            similarities = []\n",
    "            for images, input_ids, attention_mask in tqdm(dataloader, desc=f\"{perturbation_type} {level}\"):\n",
    "                images = images.to(device)\n",
    "                images_perturbed = torch.stack([perturbation_fn(img, level) for img in images])\n",
    "                input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    vision_embeds, text_embeds, _ = model(images_perturbed, input_ids, attention_mask)\n",
    "                    vision_embeds = vision_embeds / vision_embeds.norm(dim=-1, keepdim=True)\n",
    "                    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "                    logits = torch.matmul(vision_embeds, text_embeds.T)\n",
    "                    labels = torch.arange(len(images)).to(device)\n",
    "                    \n",
    "                    _, pred_i2t = logits.topk(1, dim=1)\n",
    "                    correct_i2t = pred_i2t.eq(labels.view(-1, 1)).sum().item()\n",
    "                    _, pred_t2i = logits.T.topk(1, dim=1)\n",
    "                    correct_t2i = pred_t2i.eq(labels.view(-1, 1)).sum().item()\n",
    "                    \n",
    "                    top1_correct += correct_i2t + correct_t2i\n",
    "                    total += len(images) * 2\n",
    "                    sim = logits.diag().cpu().numpy()\n",
    "                    similarities.extend(sim)\n",
    "            \n",
    "            level_results[f\"{perturbation_type}_{level}\"] = {\n",
    "                \"top1_accuracy\": top1_correct / total,\n",
    "                \"top1_error\": 1 - (top1_correct / total),\n",
    "                \"avg_similarity\": np.mean(similarities)\n",
    "            }\n",
    "        return level_results\n",
    "    \n",
    "    results[\"noise_results\"] = evaluate_perturbed(dataloader, add_gaussian_noise, noise_levels, \"noise\")\n",
    "    results[\"blur_results\"] = evaluate_perturbed(dataloader, add_gaussian_blur, blur_levels, \"blur\")\n",
    "    results[\"occlusion_results\"] = evaluate_perturbed(dataloader, add_occlusion, [32, 64], \"occlusion\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Interpretability Analysis\n",
    "def interpretability_analysis(model, dataloader, device, model_name, model_type, num_samples=5):\n",
    "    model.eval()\n",
    "    attention_maps = []\n",
    "    entropies = []\n",
    "    num_patches = 196  # Default for 224x224 images with 16x16 patches\n",
    "    grid_size = int(np.sqrt(num_patches))  # 14 for 196 patches\n",
    "\n",
    "    for i, (images, input_ids, attention_mask) in enumerate(dataloader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            vision_embeds, text_embeds, attentions = model(images, input_ids, attention_mask)\n",
    "            attn_weights = attentions[-1]  # Shape: (batch, num_heads, seq_len, seq_len)\n",
    "            attn_weights = attn_weights.mean(dim=1)  # Average over heads: (batch, seq_len, seq_len)\n",
    "\n",
    "            if model_type == \"vilt\":\n",
    "                # ViLT: seq_len = 1 (CLS) + 196 (image patches) + text_tokens\n",
    "                # Take attention to image patches only (indices 1 to 196)\n",
    "                cls_attn = attn_weights[:, 0, 1:197]  # Shape: (batch, 196)\n",
    "            else:\n",
    "                # CLIP, BLIP, from-scratch: seq_len = 1 (CLS) + 196 (patches)\n",
    "                cls_attn = attn_weights[:, 0, 1:]  # Shape: (batch, 196)\n",
    "\n",
    "            # Debug: Print shape to confirm\n",
    "            print(f\"{model_name} cls_attn shape: {cls_attn.shape}\")\n",
    "\n",
    "            # Ensure cls_attn has the expected number of patches\n",
    "            if cls_attn.shape[1] != num_patches:\n",
    "                print(f\"Warning: Expected {num_patches} patches, got {cls_attn.shape[1]}. Skipping visualization for {model_name}.\")\n",
    "                continue\n",
    "\n",
    "            # Compute attention entropy\n",
    "            attn_probs = cls_attn.cpu().numpy()\n",
    "            sample_entropy = [entropy(probs) for probs in attn_probs if probs.sum() > 0]\n",
    "            entropies.extend(sample_entropy)\n",
    "            \n",
    "            # Reshape attention for visualization\n",
    "            attn_map = cls_attn.view(-1, grid_size, grid_size).cpu().numpy()\n",
    "            attention_maps.append(attn_map)\n",
    "    \n",
    "    # Save attention visualization\n",
    "    if attention_maps:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, attn_map in enumerate(attention_maps[:num_samples]):\n",
    "            plt.subplot(1, num_samples, i + 1)\n",
    "            sns.heatmap(attn_map[0], cmap=\"viridis\")\n",
    "            plt.title(f\"Sample {i+1}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{model_name}_attention_maps.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"avg_attention_entropy\": np.mean(entropies) if entropies else 0.0,\n",
    "        \"attention_maps_path\": f\"{model_name}_attention_maps.png\" if attention_maps else \"None\"\n",
    "    }\n",
    "\n",
    "# Fairness Analysis\n",
    "def fairness_analysis(model, dataloader, device, model_name, model_type, metadata_csv=\"coco_metadata.csv\"):\n",
    "    model.eval()\n",
    "    metadata = pd.read_csv(metadata_csv)\n",
    "    subgroups = metadata[\"category\"].unique()\n",
    "    fairness_results = {sg: {\"top1_correct\": 0, \"total\": 0, \"similarities\": []} for sg in subgroups}\n",
    "    \n",
    "    data_idx = 0\n",
    "    for images, input_ids, attention_mask in tqdm(dataloader, desc=f\"Fairness {model_name}\"):\n",
    "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            vision_embeds, text_embeds, _ = model(images, input_ids, attention_mask)\n",
    "            vision_embeds = vision_embeds / vision_embeds.norm(dim=-1, keepdim=True)\n",
    "            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "            logits = torch.matmul(vision_embeds, text_embeds.T)\n",
    "            labels = torch.arange(len(images)).to(device)\n",
    "            \n",
    "            _, pred_i2t = logits.topk(1, dim=1)\n",
    "            correct_i2t = pred_i2t.eq(labels.view(-1, 1)).sum().item()\n",
    "            _, pred_t2i = logits.T.topk(1, dim=1)\n",
    "            correct_t2i = pred_t2i.eq(labels.view(-1, 1)).sum().item()\n",
    "            sim = logits.diag().cpu().numpy()\n",
    "            \n",
    "            for idx in range(len(images)):\n",
    "                if data_idx >= len(metadata):\n",
    "                    break\n",
    "                sg = metadata.iloc[data_idx][\"category\"]\n",
    "                fairness_results[sg][\"top1_correct\"] += (correct_i2t + correct_t2i) / 2\n",
    "                fairness_results[sg][\"total\"] += 1\n",
    "                fairness_results[sg][\"similarities\"].append(sim[idx])\n",
    "                data_idx += 1\n",
    "    \n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"subgroup_metrics\": {}\n",
    "    }\n",
    "    for sg in subgroups:\n",
    "        total = fairness_results[sg][\"total\"]\n",
    "        if total > 0:\n",
    "            results[\"subgroup_metrics\"][sg] = {\n",
    "                \"top1_accuracy\": fairness_results[sg][\"top1_correct\"] / total,\n",
    "                \"top1_error\": 1 - (fairness_results[sg][\"top1_correct\"] / total),\n",
    "                \"avg_similarity\": np.mean(fairness_results[sg][\"similarities\"])\n",
    "            }\n",
    "    \n",
    "    accuracies = [results[\"subgroup_metrics\"][sg][\"top1_accuracy\"] for sg in subgroups if sg in results[\"subgroup_metrics\"]]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=subgroups, y=accuracies)\n",
    "    plt.title(f\"{model_name} Fairness Across Subgroups\")\n",
    "    plt.ylabel(\"Top-1 Accuracy\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{model_name}_fairness_plot.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    results[\"fairness_plot_path\"] = f\"{model_name}_fairness_plot.png\"\n",
    "    return results\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    models = [\n",
    "        (\"CLIP\", CLIP, \"clip_model.pth\", \"clip\"),\n",
    "        (\"BLIP\", BLIP, \"blip_model.pth\", \"blip\"),\n",
    "        (\"VILT\", VILT, \"vilt_model.pth\", \"vilt\"),\n",
    "        (\"CLIPFromScratch\", CLIPFromScratch, \"clip_from_scratch.pth\", \"clip\"),\n",
    "        (\"BLIPFromScratch\", BLIPFromScratch, \"blip_from_scratch.pth\", \"clip\"),\n",
    "        (\"VILTFromScratch\", VILTFromScratch, \"vilt_from_scratch.pth\", \"vilt\")\n",
    "    ]\n",
    "\n",
    "    all_results = []\n",
    "    for model_name, model_class, model_path, model_type in models:\n",
    "        print(f\"\\nProcessing {model_name}...\")\n",
    "        dataset = CocoDataset(\"coco_dataset.csv\", model_type=model_type)\n",
    "        dataloader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "        \n",
    "        model = model_class().to(device)\n",
    "        if os.path.exists(model_path):\n",
    "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        else:\n",
    "            print(f\"Warning: {model_path} not found. Using untrained model.\")\n",
    "        \n",
    "        robustness_result = robustness_test(model, dataloader, device, model_name, model_type)\n",
    "        all_results.append(robustness_result)\n",
    "        \n",
    "        interpretability_result = interpretability_analysis(model, dataloader, device, model_name, model_type)\n",
    "        all_results.append(interpretability_result)\n",
    "        \n",
    "        fairness_result = fairness_analysis(model, dataloader, device, model_name, model_type)\n",
    "        all_results.append(fairness_result)\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv(\"robustness_interpretability_fairness_results.csv\", index=False)\n",
    "    print(\"\\nResults saved to 'robustness_interpretability_fairness_results.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93cea589-1f92-40b2-8215-d7b2c3e5924b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed coco_images/000000365426.jpg from dataset and metadata.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset and metadata\n",
    "dataset_df = pd.read_csv(\"coco_dataset.csv\")\n",
    "metadata_df = pd.read_csv(\"coco_metadata.csv\")\n",
    "\n",
    "# Filter out the corrupted image\n",
    "corrupted_image = \"coco_images/000000365426.jpg\"\n",
    "dataset_df = dataset_df[dataset_df[\"image_path\"] != corrupted_image]\n",
    "metadata_df = metadata_df[metadata_df[\"image_path\"] != corrupted_image]\n",
    "\n",
    "# Save updated files\n",
    "dataset_df.to_csv(\"coco_dataset.csv\", index=False)\n",
    "metadata_df.to_csv(\"coco_metadata.csv\", index=False)\n",
    "print(f\"Removed {corrupted_image} from dataset and metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a08ddc7-e905-4024-8506-011920597b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
